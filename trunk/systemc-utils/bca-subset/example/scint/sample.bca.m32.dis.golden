
sample.o:     file format elf32-i386

Disassembly of section .text:

<cnv_si_s13(int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_ui_s13(unsigned int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_sl_s13(long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_ul_s13(unsigned long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s12_s13(sc_dt::sc_int<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s13_s13(sc_dt::sc_int<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s14_s13(sc_dt::sc_int<14>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s31_s13(sc_dt::sc_int<31>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s32_s13(sc_dt::sc_int<32>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s33_s13(sc_dt::sc_int<33>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s43_s13(sc_dt::sc_int<43>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s63_s13(sc_dt::sc_int<63>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s64_s13(sc_dt::sc_int<64>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_si_u13(int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_ui_u13(unsigned int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_sl_u13(long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_ul_u13(unsigned long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u12_u13(sc_dt::sc_uint<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u13_u13(sc_dt::sc_uint<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u14_u13(sc_dt::sc_uint<14>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u31_u13(sc_dt::sc_uint<31>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u32_u13(sc_dt::sc_uint<32>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u33_u13(sc_dt::sc_uint<33>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u43_u13(sc_dt::sc_uint<43>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u63_u13(sc_dt::sc_uint<63>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u64_u13(sc_dt::sc_uint<64>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_si_s32(int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_ui_s32(unsigned int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_sl_s32(long long)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_ul_s32(unsigned long long)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s13_s32(sc_dt::sc_int<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s31_s32(sc_dt::sc_int<31>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s32_s32(sc_dt::sc_int<32>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s33_s32(sc_dt::sc_int<33>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s43_s32(sc_dt::sc_int<43>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s63_s32(sc_dt::sc_int<63>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s64_s32(sc_dt::sc_int<64>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_si_u32(int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_ui_u32(unsigned int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_sl_u32(long long)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_ul_u32(unsigned long long)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u13_u32(sc_dt::sc_uint<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u31_u32(sc_dt::sc_uint<31>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u32_u32(sc_dt::sc_uint<32>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u33_u32(sc_dt::sc_uint<33>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u43_u32(sc_dt::sc_uint<43>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u63_u32(sc_dt::sc_uint<63>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_u64_u32(sc_dt::sc_uint<64>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_si_s43(int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_ui_s43(unsigned int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_sl_s43(long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
0f a4 d1 15          	shld   $0x15,%edx,%ecx
c1 e2 15             	shl    $0x15,%edx
0f ac ca 15          	shrd   $0x15,%ecx,%edx
c1 f9 15             	sar    $0x15,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_ul_s43(unsigned long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
0f a4 d1 15          	shld   $0x15,%edx,%ecx
c1 e2 15             	shl    $0x15,%edx
0f ac ca 15          	shrd   $0x15,%ecx,%edx
c1 f9 15             	sar    $0x15,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_s13_s43(sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s31_s43(sc_dt::sc_int<31>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s32_s43(sc_dt::sc_int<32>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s33_s43(sc_dt::sc_int<33>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
0f a4 d1 15          	shld   $0x15,%edx,%ecx
c1 e2 15             	shl    $0x15,%edx
0f ac ca 15          	shrd   $0x15,%ecx,%edx
c1 f9 15             	sar    $0x15,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_s42_s43(sc_dt::sc_int<42>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
0f a4 d1 15          	shld   $0x15,%edx,%ecx
c1 e2 15             	shl    $0x15,%edx
0f ac ca 15          	shrd   $0x15,%ecx,%edx
c1 f9 15             	sar    $0x15,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_s43_s43(sc_dt::sc_int<43>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_s44_s43(sc_dt::sc_int<44>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
0f a4 d1 15          	shld   $0x15,%edx,%ecx
c1 e2 15             	shl    $0x15,%edx
0f ac ca 15          	shrd   $0x15,%ecx,%edx
c1 f9 15             	sar    $0x15,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_s63_s43(sc_dt::sc_int<63>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
0f a4 d1 15          	shld   $0x15,%edx,%ecx
c1 e2 15             	shl    $0x15,%edx
0f ac ca 15          	shrd   $0x15,%ecx,%edx
c1 f9 15             	sar    $0x15,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_s64_s43(sc_dt::sc_int<64>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
0f a4 d1 15          	shld   $0x15,%edx,%ecx
c1 e2 15             	shl    $0x15,%edx
0f ac ca 15          	shrd   $0x15,%ecx,%edx
c1 f9 15             	sar    $0x15,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_si_u43(int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 ca                	mov    %ecx,%edx
81 e2 ff 07 00 00    	and    $0x7ff,%edx
89 50 04             	mov    %edx,0x4(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_ui_u43(unsigned int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_sl_u43(long long)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 ca                	mov    %ecx,%edx
81 e2 ff 07 00 00    	and    $0x7ff,%edx
89 50 04             	mov    %edx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_ul_u43(unsigned long long)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 ca                	mov    %ecx,%edx
81 e2 ff 07 00 00    	and    $0x7ff,%edx
89 50 04             	mov    %edx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_u13_u43(sc_dt::sc_uint<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_u31_u43(sc_dt::sc_uint<31>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_u32_u43(sc_dt::sc_uint<32>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_u33_u43(sc_dt::sc_uint<33>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_u42_u43(sc_dt::sc_uint<42>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_u43_u43(sc_dt::sc_uint<43>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_u44_u43(sc_dt::sc_uint<44>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 10                	mov    %edx,(%eax)
8b 54 24 0c          	mov    0xc(%esp),%edx
81 e2 ff 07 00 00    	and    $0x7ff,%edx
89 50 04             	mov    %edx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_u63_u43(sc_dt::sc_uint<63>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 10                	mov    %edx,(%eax)
8b 54 24 0c          	mov    0xc(%esp),%edx
81 e2 ff 07 00 00    	and    $0x7ff,%edx
89 50 04             	mov    %edx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_u64_u43(sc_dt::sc_uint<64>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 10                	mov    %edx,(%eax)
8b 54 24 0c          	mov    0xc(%esp),%edx
81 e2 ff 07 00 00    	and    $0x7ff,%edx
89 50 04             	mov    %edx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_si_s64(int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_ui_s64(unsigned int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_sl_s64(long long)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_ul_s64(unsigned long long)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_s13_s64(sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s31_s64(sc_dt::sc_int<31>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s32_s64(sc_dt::sc_int<32>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_s33_s64(sc_dt::sc_int<33>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_s43_s64(sc_dt::sc_int<43>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_s63_s64(sc_dt::sc_int<63>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_s64_s64(sc_dt::sc_int<64>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_si_u64(int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_ui_u64(unsigned int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_sl_u64(long long)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_ul_u64(unsigned long long)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_u13_u64(sc_dt::sc_uint<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_u31_u64(sc_dt::sc_uint<31>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_u32_u64(sc_dt::sc_uint<32>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_u33_u64(sc_dt::sc_uint<33>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_u43_u64(sc_dt::sc_uint<43>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_u63_u64(sc_dt::sc_uint<63>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_u64_u64(sc_dt::sc_uint<64>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_b_s1(bool)>:
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 1f             	shl    $0x1f,%edx
c1 fa 1f             	sar    $0x1f,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_b_u1(bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_b_s2(bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_b_u2(bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_b_s32(bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_b_u32(bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_b_s43(bool)>:
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_b_u43(bool)>:
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_b_s64(bool)>:
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_b_u64(bool)>:
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_sc_s7(signed char)>:
0f be 54 24 08       	movsbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 19             	shl    $0x19,%edx
c1 fa 19             	sar    $0x19,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_sc_u7(signed char)>:
0f be 54 24 08       	movsbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
83 e2 7f             	and    $0x7f,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_sc_s8(signed char)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 54 24 08       	movsbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_sc_u8(signed char)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_sc_s9(signed char)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 54 24 08       	movsbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_sc_u9(signed char)>:
0f be 54 24 08       	movsbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 01 00 00    	and    $0x1ff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_sc_s32(signed char)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 54 24 08       	movsbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_sc_u32(signed char)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 54 24 08       	movsbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_sc_s43(signed char)>:
0f be 54 24 08       	movsbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_sc_u43(signed char)>:
0f be 54 24 08       	movsbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 ca                	mov    %ecx,%edx
81 e2 ff 07 00 00    	and    $0x7ff,%edx
89 50 04             	mov    %edx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_sc_s64(signed char)>:
0f be 54 24 08       	movsbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_sc_u64(signed char)>:
0f be 54 24 08       	movsbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_uc_s7(unsigned char)>:
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 19             	shl    $0x19,%edx
c1 fa 19             	sar    $0x19,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_uc_u7(unsigned char)>:
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
83 e2 7f             	and    $0x7f,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<cnv_uc_s8(unsigned char)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 54 24 08       	movsbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_uc_u8(unsigned char)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_uc_s9(unsigned char)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_uc_u9(unsigned char)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_uc_s32(unsigned char)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_uc_u32(unsigned char)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_uc_s43(unsigned char)>:
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_uc_u43(unsigned char)>:
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_uc_s64(unsigned char)>:
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_uc_u64(unsigned char)>:
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_ss_s15(short)>:
0f bf 54 24 08       	movswl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 11             	shl    $0x11,%edx
c1 fa 11             	sar    $0x11,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_ss_u15(short)>:
0f bf 54 24 08       	movswl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 7f 00 00    	and    $0x7fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_ss_s16(short)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 54 24 08       	movswl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_ss_u16(short)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 54 24 08       	movzwl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_ss_s17(short)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 54 24 08       	movswl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_ss_u17(short)>:
0f bf 54 24 08       	movswl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff ff 01 00    	and    $0x1ffff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_ss_s32(short)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 54 24 08       	movswl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_ss_u32(short)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 54 24 08       	movswl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_ss_s43(short)>:
0f bf 54 24 08       	movswl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_ss_u43(short)>:
0f bf 54 24 08       	movswl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 ca                	mov    %ecx,%edx
81 e2 ff 07 00 00    	and    $0x7ff,%edx
89 50 04             	mov    %edx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_ss_s64(short)>:
0f bf 54 24 08       	movswl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_ss_u64(short)>:
0f bf 54 24 08       	movswl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_us_s15(unsigned short)>:
0f b7 54 24 08       	movzwl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 11             	shl    $0x11,%edx
c1 fa 11             	sar    $0x11,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_us_u15(unsigned short)>:
0f b7 54 24 08       	movzwl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
81 e2 ff 7f 00 00    	and    $0x7fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_us_s16(unsigned short)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 54 24 08       	movswl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_us_u16(unsigned short)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 54 24 08       	movzwl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_us_s17(unsigned short)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 54 24 08       	movzwl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_us_u17(unsigned short)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 54 24 08       	movzwl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_us_s32(unsigned short)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 54 24 08       	movzwl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_us_u32(unsigned short)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 54 24 08       	movzwl 0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<cnv_us_s43(unsigned short)>:
0f b7 54 24 08       	movzwl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_us_u43(unsigned short)>:
0f b7 54 24 08       	movzwl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_us_s64(unsigned short)>:
0f b7 54 24 08       	movzwl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<cnv_us_u64(unsigned short)>:
0f b7 54 24 08       	movzwl 0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<ass_si_s13(int const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_ui_s13(unsigned int const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_sl_s13(long long const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_ul_s13(unsigned long long const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_s12_s13(sc_dt::sc_int<12> const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_s13_s13(sc_dt::sc_int<13> const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_s14_s13(sc_dt::sc_int<14> const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_s31_s13(sc_dt::sc_int<31> const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_s32_s13(sc_dt::sc_int<32> const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_s33_s13(sc_dt::sc_int<33> const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_s43_s13(sc_dt::sc_int<43> const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_s63_s13(sc_dt::sc_int<63> const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_s64_s13(sc_dt::sc_int<64> const&, sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_si_u13(int const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_ui_u13(unsigned int const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_sl_u13(long long const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_ul_u13(unsigned long long const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u12_u13(sc_dt::sc_uint<12> const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u13_u13(sc_dt::sc_uint<13> const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u14_u13(sc_dt::sc_uint<14> const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u31_u13(sc_dt::sc_uint<31> const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u32_u13(sc_dt::sc_uint<32> const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u33_u13(sc_dt::sc_uint<33> const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u43_u13(sc_dt::sc_uint<43> const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u63_u13(sc_dt::sc_uint<63> const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u64_u13(sc_dt::sc_uint<64> const&, sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_si_s32(int const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_ui_s32(unsigned int const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_sl_s32(long long const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_ul_s32(unsigned long long const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_s13_s32(sc_dt::sc_int<13> const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_s31_s32(sc_dt::sc_int<31> const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_s32_s32(sc_dt::sc_int<32> const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_s33_s32(sc_dt::sc_int<33> const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_s43_s32(sc_dt::sc_int<43> const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_s63_s32(sc_dt::sc_int<63> const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_s64_s32(sc_dt::sc_int<64> const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_si_u32(int const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_ui_u32(unsigned int const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_sl_u32(long long const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_ul_u32(unsigned long long const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u13_u32(sc_dt::sc_uint<13> const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u31_u32(sc_dt::sc_uint<31> const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u32_u32(sc_dt::sc_uint<32> const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u33_u32(sc_dt::sc_uint<33> const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u43_u32(sc_dt::sc_uint<43> const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u63_u32(sc_dt::sc_uint<63> const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_u64_u32(sc_dt::sc_uint<64> const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    
90                   	nop    

<ass_si_s43(int const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    
90                   	nop    

<ass_ui_s43(unsigned int const&, sc_dt::sc_int<43>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 0a                	mov    (%edx),%ecx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 08                	mov    %ecx,(%eax)
c3                   	ret    

<ass_sl_s43(long long const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 01                	mov    %eax,(%ecx)
89 51 04             	mov    %edx,0x4(%ecx)
c3                   	ret    
90                   	nop    

<ass_ul_s43(unsigned long long const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 01                	mov    %eax,(%ecx)
89 51 04             	mov    %edx,0x4(%ecx)
c3                   	ret    
90                   	nop    

<ass_s13_s43(sc_dt::sc_int<13> const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    
90                   	nop    

<ass_s31_s43(sc_dt::sc_int<31> const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    
90                   	nop    

<ass_s32_s43(sc_dt::sc_int<32> const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    
90                   	nop    

<ass_s33_s43(sc_dt::sc_int<33> const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 01                	mov    %eax,(%ecx)
89 51 04             	mov    %edx,0x4(%ecx)
c3                   	ret    
90                   	nop    

<ass_s42_s43(sc_dt::sc_int<42> const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 01                	mov    %eax,(%ecx)
89 51 04             	mov    %edx,0x4(%ecx)
c3                   	ret    
90                   	nop    

<ass_s43_s43(sc_dt::sc_int<43> const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_s44_s43(sc_dt::sc_int<44> const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 01                	mov    %eax,(%ecx)
89 51 04             	mov    %edx,0x4(%ecx)
c3                   	ret    
90                   	nop    

<ass_s63_s43(sc_dt::sc_int<63> const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 01                	mov    %eax,(%ecx)
89 51 04             	mov    %edx,0x4(%ecx)
c3                   	ret    
90                   	nop    

<ass_s64_s43(sc_dt::sc_int<64> const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 01                	mov    %eax,(%ecx)
89 51 04             	mov    %edx,0x4(%ecx)
c3                   	ret    
90                   	nop    

<ass_si_u43(int const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
89 01                	mov    %eax,(%ecx)
89 d0                	mov    %edx,%eax
25 ff 07 00 00       	and    $0x7ff,%eax
89 41 04             	mov    %eax,0x4(%ecx)
c3                   	ret    

<ass_ui_u43(unsigned int const&, sc_dt::sc_uint<43>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 0a                	mov    (%edx),%ecx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 08                	mov    %ecx,(%eax)
c3                   	ret    

<ass_sl_u43(long long const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 01                	mov    %eax,(%ecx)
89 d0                	mov    %edx,%eax
25 ff 07 00 00       	and    $0x7ff,%eax
89 41 04             	mov    %eax,0x4(%ecx)
c3                   	ret    

<ass_ul_u43(unsigned long long const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 01                	mov    %eax,(%ecx)
89 d0                	mov    %edx,%eax
25 ff 07 00 00       	and    $0x7ff,%eax
89 41 04             	mov    %eax,0x4(%ecx)
c3                   	ret    

<ass_u13_u43(sc_dt::sc_uint<13> const&, sc_dt::sc_uint<43>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 0a                	mov    (%edx),%ecx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 08                	mov    %ecx,(%eax)
c3                   	ret    

<ass_u31_u43(sc_dt::sc_uint<31> const&, sc_dt::sc_uint<43>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 0a                	mov    (%edx),%ecx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 08                	mov    %ecx,(%eax)
c3                   	ret    

<ass_u32_u43(sc_dt::sc_uint<32> const&, sc_dt::sc_uint<43>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 0a                	mov    (%edx),%ecx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 08                	mov    %ecx,(%eax)
c3                   	ret    

<ass_u33_u43(sc_dt::sc_uint<33> const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_u42_u43(sc_dt::sc_uint<42> const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_u43_u43(sc_dt::sc_uint<43> const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_u44_u43(sc_dt::sc_uint<44> const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 01                	mov    %eax,(%ecx)
89 d0                	mov    %edx,%eax
25 ff 07 00 00       	and    $0x7ff,%eax
89 41 04             	mov    %eax,0x4(%ecx)
c3                   	ret    

<ass_u63_u43(sc_dt::sc_uint<63> const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 01                	mov    %eax,(%ecx)
89 d0                	mov    %edx,%eax
25 ff 07 00 00       	and    $0x7ff,%eax
89 41 04             	mov    %eax,0x4(%ecx)
c3                   	ret    

<ass_u64_u43(sc_dt::sc_uint<64> const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 01                	mov    %eax,(%ecx)
89 d0                	mov    %edx,%eax
25 ff 07 00 00       	and    $0x7ff,%eax
89 41 04             	mov    %eax,0x4(%ecx)
c3                   	ret    

<ass_si_s64(int const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    
90                   	nop    

<ass_ui_s64(unsigned int const&, sc_dt::sc_int<64>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 0a                	mov    (%edx),%ecx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 08                	mov    %ecx,(%eax)
c3                   	ret    

<ass_sl_s64(long long const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_ul_s64(unsigned long long const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_s13_s64(sc_dt::sc_int<13> const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    
90                   	nop    

<ass_s31_s64(sc_dt::sc_int<31> const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    
90                   	nop    

<ass_s32_s64(sc_dt::sc_int<32> const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    
90                   	nop    

<ass_s33_s64(sc_dt::sc_int<33> const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_s43_s64(sc_dt::sc_int<43> const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_s63_s64(sc_dt::sc_int<63> const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_s64_s64(sc_dt::sc_int<64> const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_si_u64(int const&, sc_dt::sc_uint<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    
90                   	nop    

<ass_ui_u64(unsigned int const&, sc_dt::sc_uint<64>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 0a                	mov    (%edx),%ecx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 08                	mov    %ecx,(%eax)
c3                   	ret    

<ass_sl_u64(long long const&, sc_dt::sc_uint<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_ul_u64(unsigned long long const&, sc_dt::sc_uint<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_u13_u64(sc_dt::sc_uint<13> const&, sc_dt::sc_uint<64>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 0a                	mov    (%edx),%ecx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 08                	mov    %ecx,(%eax)
c3                   	ret    

<ass_u31_u64(sc_dt::sc_uint<31> const&, sc_dt::sc_uint<64>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 0a                	mov    (%edx),%ecx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 08                	mov    %ecx,(%eax)
c3                   	ret    

<ass_u32_u64(sc_dt::sc_uint<32> const&, sc_dt::sc_uint<64>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 0a                	mov    (%edx),%ecx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 08                	mov    %ecx,(%eax)
c3                   	ret    

<ass_u33_u64(sc_dt::sc_uint<33> const&, sc_dt::sc_uint<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_u43_u64(sc_dt::sc_uint<43> const&, sc_dt::sc_uint<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_u63_u64(sc_dt::sc_uint<63> const&, sc_dt::sc_uint<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_u64_u64(sc_dt::sc_uint<64> const&, sc_dt::sc_uint<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 50 04             	mov    0x4(%eax),%edx
8b 00                	mov    (%eax),%eax
89 51 04             	mov    %edx,0x4(%ecx)
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<ass_b_s1(bool const&, sc_dt::sc_int<1>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 00             	movzbl (%eax),%eax
c1 e0 1f             	shl    $0x1f,%eax
c1 f8 1f             	sar    $0x1f,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    

<ass_b_u1(bool const&, sc_dt::sc_uint<1>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_b_s2(bool const&, sc_dt::sc_int<2>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_b_u2(bool const&, sc_dt::sc_uint<2>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_b_s32(bool const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_b_u32(bool const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_b_s43(bool const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_b_u43(bool const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_b_s64(bool const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_b_u64(bool const&, sc_dt::sc_uint<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_sc_s7(signed char const&, sc_dt::sc_int<7>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
0f be 00             	movsbl (%eax),%eax
c1 e0 19             	shl    $0x19,%eax
c1 f8 19             	sar    $0x19,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    

<ass_sc_u7(signed char const&, sc_dt::sc_uint<7>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
0f be 00             	movsbl (%eax),%eax
83 e0 7f             	and    $0x7f,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_sc_s8(signed char const&, sc_dt::sc_int<8>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 10             	movsbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_sc_u8(signed char const&, sc_dt::sc_uint<8>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_sc_s9(signed char const&, sc_dt::sc_int<9>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 10             	movsbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_sc_u9(signed char const&, sc_dt::sc_uint<9>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
0f be 00             	movsbl (%eax),%eax
25 ff 01 00 00       	and    $0x1ff,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_sc_s32(signed char const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 10             	movsbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_sc_u32(signed char const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 10             	movsbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_sc_s43(signed char const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 10             	movsbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_sc_u43(signed char const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
0f be 00             	movsbl (%eax),%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
89 01                	mov    %eax,(%ecx)
89 d0                	mov    %edx,%eax
25 ff 07 00 00       	and    $0x7ff,%eax
89 41 04             	mov    %eax,0x4(%ecx)
c3                   	ret    
90                   	nop    

<ass_sc_s64(signed char const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 10             	movsbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_sc_u64(signed char const&, sc_dt::sc_uint<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 10             	movsbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_uc_s7(unsigned char const&, sc_dt::sc_int<7>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 00             	movzbl (%eax),%eax
c1 e0 19             	shl    $0x19,%eax
c1 f8 19             	sar    $0x19,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    

<ass_uc_u7(unsigned char const&, sc_dt::sc_uint<7>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 00             	movzbl (%eax),%eax
83 e0 7f             	and    $0x7f,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_uc_s8(unsigned char const&, sc_dt::sc_int<8>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f be 10             	movsbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_uc_u8(unsigned char const&, sc_dt::sc_uint<8>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_uc_s9(unsigned char const&, sc_dt::sc_int<9>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_uc_u9(unsigned char const&, sc_dt::sc_uint<9>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_uc_s32(unsigned char const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_uc_u32(unsigned char const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_uc_s43(unsigned char const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_uc_u43(unsigned char const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_uc_s64(unsigned char const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_uc_u64(unsigned char const&, sc_dt::sc_uint<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 10             	movzbl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_ss_s15(short const&, sc_dt::sc_int<15>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
0f bf 00             	movswl (%eax),%eax
c1 e0 11             	shl    $0x11,%eax
c1 f8 11             	sar    $0x11,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    

<ass_ss_u15(short const&, sc_dt::sc_uint<15>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
0f bf 00             	movswl (%eax),%eax
25 ff 7f 00 00       	and    $0x7fff,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_ss_s16(short const&, sc_dt::sc_int<16>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 10             	movswl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_ss_u16(short const&, sc_dt::sc_uint<16>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 10             	movzwl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_ss_s17(short const&, sc_dt::sc_int<17>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 10             	movswl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_ss_u17(short const&, sc_dt::sc_uint<17>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
0f bf 00             	movswl (%eax),%eax
25 ff ff 01 00       	and    $0x1ffff,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_ss_s32(short const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 10             	movswl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_ss_u32(short const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 10             	movswl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_ss_s43(short const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 10             	movswl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_ss_u43(short const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
0f bf 00             	movswl (%eax),%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
89 01                	mov    %eax,(%ecx)
89 d0                	mov    %edx,%eax
25 ff 07 00 00       	and    $0x7ff,%eax
89 41 04             	mov    %eax,0x4(%ecx)
c3                   	ret    
90                   	nop    

<ass_ss_s64(short const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 10             	movswl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_ss_u64(short const&, sc_dt::sc_uint<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 10             	movswl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_us_s15(unsigned short const&, sc_dt::sc_int<15>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
0f b7 00             	movzwl (%eax),%eax
c1 e0 11             	shl    $0x11,%eax
c1 f8 11             	sar    $0x11,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    

<ass_us_u15(unsigned short const&, sc_dt::sc_uint<15>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
0f b7 00             	movzwl (%eax),%eax
25 ff 7f 00 00       	and    $0x7fff,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<ass_us_s16(unsigned short const&, sc_dt::sc_int<16>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f bf 10             	movswl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_us_u16(unsigned short const&, sc_dt::sc_uint<16>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 10             	movzwl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_us_s17(unsigned short const&, sc_dt::sc_int<17>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 10             	movzwl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_us_u17(unsigned short const&, sc_dt::sc_uint<17>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 10             	movzwl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_us_s32(unsigned short const&, sc_dt::sc_int<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 10             	movzwl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_us_u32(unsigned short const&, sc_dt::sc_uint<32>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 10             	movzwl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 10                	mov    %edx,(%eax)
c3                   	ret    

<ass_us_s43(unsigned short const&, sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 10             	movzwl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_us_u43(unsigned short const&, sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 10             	movzwl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_us_s64(unsigned short const&, sc_dt::sc_int<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 10             	movzwl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<ass_us_u64(unsigned short const&, sc_dt::sc_uint<64>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b7 10             	movzwl (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c3                   	ret    

<add_s13_s13_s13(sc_dt::sc_int<13>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_s13_s14(sc_dt::sc_int<13>, sc_dt::sc_int<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_s12_s13(sc_dt::sc_int<13>, sc_dt::sc_int<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_s12_s14(sc_dt::sc_int<13>, sc_dt::sc_int<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_s13_s13(sc_dt::sc_int<12>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_s13_s14(sc_dt::sc_int<12>, sc_dt::sc_int<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_s12_s13(sc_dt::sc_int<12>, sc_dt::sc_int<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_s12_s14(sc_dt::sc_int<12>, sc_dt::sc_int<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_u13_s13(sc_dt::sc_int<13>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_u13_s14(sc_dt::sc_int<13>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_u12_s13(sc_dt::sc_int<13>, sc_dt::sc_uint<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_u12_s14(sc_dt::sc_int<13>, sc_dt::sc_uint<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_u13_s13(sc_dt::sc_int<12>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_u13_s14(sc_dt::sc_int<12>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_u12_s13(sc_dt::sc_int<12>, sc_dt::sc_uint<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_u12_s14(sc_dt::sc_int<12>, sc_dt::sc_uint<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_s13_s13(sc_dt::sc_uint<13>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_s13_s14(sc_dt::sc_uint<13>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_s12_s13(sc_dt::sc_uint<13>, sc_dt::sc_int<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_s12_s14(sc_dt::sc_uint<13>, sc_dt::sc_int<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_s13_s13(sc_dt::sc_uint<12>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_s13_s14(sc_dt::sc_uint<12>, sc_dt::sc_int<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_s12_s13(sc_dt::sc_uint<12>, sc_dt::sc_int<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_s12_s14(sc_dt::sc_uint<12>, sc_dt::sc_int<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_u13_s13(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_u13_s14(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_u12_s13(sc_dt::sc_uint<13>, sc_dt::sc_uint<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_u12_s14(sc_dt::sc_uint<13>, sc_dt::sc_uint<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_u13_s13(sc_dt::sc_uint<12>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_u13_s14(sc_dt::sc_uint<12>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_u12_s13(sc_dt::sc_uint<12>, sc_dt::sc_uint<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_u12_s14(sc_dt::sc_uint<12>, sc_dt::sc_uint<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_s13_u13(sc_dt::sc_int<13>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_s13_u14(sc_dt::sc_int<13>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_s12_u13(sc_dt::sc_int<13>, sc_dt::sc_int<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_s12_u14(sc_dt::sc_int<13>, sc_dt::sc_int<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_s13_u13(sc_dt::sc_int<12>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_s13_u14(sc_dt::sc_int<12>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_s12_u13(sc_dt::sc_int<12>, sc_dt::sc_int<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_s12_u14(sc_dt::sc_int<12>, sc_dt::sc_int<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_u13_u13(sc_dt::sc_int<13>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_u13_u14(sc_dt::sc_int<13>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_u12_u13(sc_dt::sc_int<13>, sc_dt::sc_uint<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_u12_u14(sc_dt::sc_int<13>, sc_dt::sc_uint<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_u13_u13(sc_dt::sc_int<12>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_u13_u14(sc_dt::sc_int<12>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_u12_u13(sc_dt::sc_int<12>, sc_dt::sc_uint<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s12_u12_u14(sc_dt::sc_int<12>, sc_dt::sc_uint<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_s13_u13(sc_dt::sc_uint<13>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_s13_u14(sc_dt::sc_uint<13>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_s12_u13(sc_dt::sc_uint<13>, sc_dt::sc_int<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_s12_u14(sc_dt::sc_uint<13>, sc_dt::sc_int<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_s13_u13(sc_dt::sc_uint<12>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_s13_u14(sc_dt::sc_uint<12>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_s12_u13(sc_dt::sc_uint<12>, sc_dt::sc_int<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_s12_u14(sc_dt::sc_uint<12>, sc_dt::sc_int<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_u13_u13(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_u13_u14(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_u12_u13(sc_dt::sc_uint<13>, sc_dt::sc_uint<12>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_u12_u14(sc_dt::sc_uint<13>, sc_dt::sc_uint<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_u13_u13(sc_dt::sc_uint<12>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_u13_u14(sc_dt::sc_uint<12>, sc_dt::sc_uint<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_u12_u13(sc_dt::sc_uint<12>, sc_dt::sc_uint<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u12_u12_u14(sc_dt::sc_uint<12>, sc_dt::sc_uint<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s32_s32_s32(sc_dt::sc_int<32>, sc_dt::sc_int<32>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s32_s32_s33(sc_dt::sc_int<32>, sc_dt::sc_int<32>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u32_u32_u32(sc_dt::sc_uint<32>, sc_dt::sc_uint<32>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u32_u32_u33(sc_dt::sc_uint<32>, sc_dt::sc_uint<32>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
c7 40 04 00 00 00 00 	movl   $0x0,0x4(%eax)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<add_s43_s43_s43(sc_dt::sc_int<43>, sc_dt::sc_int<43>)>:
8b 54 24 10          	mov    0x10(%esp),%edx
8b 4c 24 14          	mov    0x14(%esp),%ecx
03 54 24 08          	add    0x8(%esp),%edx
13 4c 24 0c          	adc    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
0f a4 d1 15          	shld   $0x15,%edx,%ecx
c1 e2 15             	shl    $0x15,%edx
0f ac ca 15          	shrd   $0x15,%ecx,%edx
c1 f9 15             	sar    $0x15,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<add_s43_s43_s44(sc_dt::sc_int<43>, sc_dt::sc_int<43>)>:
8b 54 24 10          	mov    0x10(%esp),%edx
8b 4c 24 14          	mov    0x14(%esp),%ecx
03 54 24 08          	add    0x8(%esp),%edx
13 4c 24 0c          	adc    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
0f a4 d1 14          	shld   $0x14,%edx,%ecx
c1 e2 14             	shl    $0x14,%edx
0f ac ca 14          	shrd   $0x14,%ecx,%edx
c1 f9 14             	sar    $0x14,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<add_u43_u43_u43(sc_dt::sc_uint<43>, sc_dt::sc_uint<43>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 10          	mov    0x10(%esp),%edx
8b 4c 24 14          	mov    0x14(%esp),%ecx
03 54 24 08          	add    0x8(%esp),%edx
13 4c 24 0c          	adc    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 ca                	mov    %ecx,%edx
81 e2 ff 07 00 00    	and    $0x7ff,%edx
89 50 04             	mov    %edx,0x4(%eax)
c2 04 00             	ret    $0x4

<add_u43_u43_u44(sc_dt::sc_uint<43>, sc_dt::sc_uint<43>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 10          	mov    0x10(%esp),%edx
8b 4c 24 14          	mov    0x14(%esp),%ecx
03 54 24 08          	add    0x8(%esp),%edx
13 4c 24 0c          	adc    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<add_s64_s64_s64(sc_dt::sc_int<64>, sc_dt::sc_int<64>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 10          	mov    0x10(%esp),%edx
8b 4c 24 14          	mov    0x14(%esp),%ecx
03 54 24 08          	add    0x8(%esp),%edx
13 4c 24 0c          	adc    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<add_u64_u64_u64(sc_dt::sc_uint<64>, sc_dt::sc_uint<64>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 10          	mov    0x10(%esp),%edx
8b 4c 24 14          	mov    0x14(%esp),%ecx
03 54 24 08          	add    0x8(%esp),%edx
13 4c 24 0c          	adc    0xc(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<add_s13_s13_1_s13(sc_dt::sc_int<13>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
83 c2 01             	add    $0x1,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<add_s13_s13_1_s14(sc_dt::sc_int<13>, sc_dt::sc_int<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
83 c2 01             	add    $0x1,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<add_s13_s13_b_s13(sc_dt::sc_int<13>, sc_dt::sc_int<13>, bool)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
0f b6 4c 24 10       	movzbl 0x10(%esp),%ecx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
01 ca                	add    %ecx,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<add_s13_s13_b_s14(sc_dt::sc_int<13>, sc_dt::sc_int<13>, bool)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
0f b6 4c 24 10       	movzbl 0x10(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
01 ca                	add    %ecx,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<add_s13_s13_u1_s13(sc_dt::sc_int<13>, sc_dt::sc_int<13>, sc_dt::sc_uint<1>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
03 54 24 10          	add    0x10(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_s13_s13_u1_s14(sc_dt::sc_int<13>, sc_dt::sc_int<13>, sc_dt::sc_uint<1>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
03 54 24 10          	add    0x10(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_u13_1_u13(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
83 c2 01             	add    $0x1,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<add_u13_u13_1_u14(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
83 c2 01             	add    $0x1,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<add_u13_u13_b_u13(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>, bool)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
0f b6 4c 24 10       	movzbl 0x10(%esp),%ecx
03 54 24 08          	add    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
01 ca                	add    %ecx,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<add_u13_u13_b_u14(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>, bool)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
0f b6 4c 24 10       	movzbl 0x10(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
01 ca                	add    %ecx,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<add_u13_u13_u1_u13(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>, sc_dt::sc_uint<1>)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
03 54 24 08          	add    0x8(%esp),%edx
03 54 24 10          	add    0x10(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<add_u13_u13_u1_u14(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>, sc_dt::sc_uint<1>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 0c          	mov    0xc(%esp),%edx
03 54 24 08          	add    0x8(%esp),%edx
03 54 24 10          	add    0x10(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_s13_s13(sc_dt::sc_int<13>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_s13_s14(sc_dt::sc_int<13>, sc_dt::sc_int<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_s12_s13(sc_dt::sc_int<13>, sc_dt::sc_int<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_s12_s14(sc_dt::sc_int<13>, sc_dt::sc_int<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_s13_s13(sc_dt::sc_int<12>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_s13_s14(sc_dt::sc_int<12>, sc_dt::sc_int<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_s12_s13(sc_dt::sc_int<12>, sc_dt::sc_int<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_s12_s14(sc_dt::sc_int<12>, sc_dt::sc_int<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_u13_s13(sc_dt::sc_int<13>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_u13_s14(sc_dt::sc_int<13>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_u12_s13(sc_dt::sc_int<13>, sc_dt::sc_uint<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_u12_s14(sc_dt::sc_int<13>, sc_dt::sc_uint<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_u13_s13(sc_dt::sc_int<12>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_u13_s14(sc_dt::sc_int<12>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_u12_s13(sc_dt::sc_int<12>, sc_dt::sc_uint<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_u12_s14(sc_dt::sc_int<12>, sc_dt::sc_uint<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_s13_s13(sc_dt::sc_uint<13>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_s13_s14(sc_dt::sc_uint<13>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_s12_s13(sc_dt::sc_uint<13>, sc_dt::sc_int<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_s12_s14(sc_dt::sc_uint<13>, sc_dt::sc_int<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_s13_s13(sc_dt::sc_uint<12>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_s13_s14(sc_dt::sc_uint<12>, sc_dt::sc_int<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_s12_s13(sc_dt::sc_uint<12>, sc_dt::sc_int<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_s12_s14(sc_dt::sc_uint<12>, sc_dt::sc_int<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_u13_s13(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_u13_s14(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_u12_s13(sc_dt::sc_uint<13>, sc_dt::sc_uint<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_u12_s14(sc_dt::sc_uint<13>, sc_dt::sc_uint<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_u13_s13(sc_dt::sc_uint<12>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_u13_s14(sc_dt::sc_uint<12>, sc_dt::sc_uint<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_u12_s13(sc_dt::sc_uint<12>, sc_dt::sc_uint<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_u12_s14(sc_dt::sc_uint<12>, sc_dt::sc_uint<12>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_s13_u13(sc_dt::sc_int<13>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_s13_u14(sc_dt::sc_int<13>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_s12_u13(sc_dt::sc_int<13>, sc_dt::sc_int<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_s12_u14(sc_dt::sc_int<13>, sc_dt::sc_int<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_s13_u13(sc_dt::sc_int<12>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_s13_u14(sc_dt::sc_int<12>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_s12_u13(sc_dt::sc_int<12>, sc_dt::sc_int<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_s12_u14(sc_dt::sc_int<12>, sc_dt::sc_int<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_u13_u13(sc_dt::sc_int<13>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_u13_u14(sc_dt::sc_int<13>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_u12_u13(sc_dt::sc_int<13>, sc_dt::sc_uint<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_u12_u14(sc_dt::sc_int<13>, sc_dt::sc_uint<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_u13_u13(sc_dt::sc_int<12>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_u13_u14(sc_dt::sc_int<12>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_u12_u13(sc_dt::sc_int<12>, sc_dt::sc_uint<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s12_u12_u14(sc_dt::sc_int<12>, sc_dt::sc_uint<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_s13_u13(sc_dt::sc_uint<13>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_s13_u14(sc_dt::sc_uint<13>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_s12_u13(sc_dt::sc_uint<13>, sc_dt::sc_int<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_s12_u14(sc_dt::sc_uint<13>, sc_dt::sc_int<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_s13_u13(sc_dt::sc_uint<12>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_s13_u14(sc_dt::sc_uint<12>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_s12_u13(sc_dt::sc_uint<12>, sc_dt::sc_int<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_s12_u14(sc_dt::sc_uint<12>, sc_dt::sc_int<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_u13_u13(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_u13_u14(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_u12_u13(sc_dt::sc_uint<13>, sc_dt::sc_uint<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_u12_u14(sc_dt::sc_uint<13>, sc_dt::sc_uint<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_u13_u13(sc_dt::sc_uint<12>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_u13_u14(sc_dt::sc_uint<12>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_u12_u13(sc_dt::sc_uint<12>, sc_dt::sc_uint<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u12_u12_u14(sc_dt::sc_uint<12>, sc_dt::sc_uint<12>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s32_s32_s32(sc_dt::sc_int<32>, sc_dt::sc_int<32>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s32_s32_s33(sc_dt::sc_int<32>, sc_dt::sc_int<32>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
89 d1                	mov    %edx,%ecx
c1 f9 1f             	sar    $0x1f,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u32_u32_u32(sc_dt::sc_uint<32>, sc_dt::sc_uint<32>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u32_u32_u33(sc_dt::sc_uint<32>, sc_dt::sc_uint<32>)>:
83 ec 08             	sub    $0x8,%esp
31 c9                	xor    %ecx,%ecx
89 1c 24             	mov    %ebx,(%esp)
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 5c 24 10          	mov    0x10(%esp),%ebx
2b 5c 24 14          	sub    0x14(%esp),%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
89 48 04             	mov    %ecx,0x4(%eax)
89 18                	mov    %ebx,(%eax)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c2 04 00             	ret    $0x4

<sub_s43_s43_s43(sc_dt::sc_int<43>, sc_dt::sc_int<43>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
2b 54 24 10          	sub    0x10(%esp),%edx
1b 4c 24 14          	sbb    0x14(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
0f a4 d1 15          	shld   $0x15,%edx,%ecx
c1 e2 15             	shl    $0x15,%edx
0f ac ca 15          	shrd   $0x15,%ecx,%edx
c1 f9 15             	sar    $0x15,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<sub_s43_s43_s44(sc_dt::sc_int<43>, sc_dt::sc_int<43>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
2b 54 24 10          	sub    0x10(%esp),%edx
1b 4c 24 14          	sbb    0x14(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
0f a4 d1 14          	shld   $0x14,%edx,%ecx
c1 e2 14             	shl    $0x14,%edx
0f ac ca 14          	shrd   $0x14,%ecx,%edx
c1 f9 14             	sar    $0x14,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<sub_u43_u43_u43(sc_dt::sc_uint<43>, sc_dt::sc_uint<43>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
2b 54 24 10          	sub    0x10(%esp),%edx
1b 4c 24 14          	sbb    0x14(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 ca                	mov    %ecx,%edx
81 e2 ff 07 00 00    	and    $0x7ff,%edx
89 50 04             	mov    %edx,0x4(%eax)
c2 04 00             	ret    $0x4

<sub_u43_u43_u44(sc_dt::sc_uint<43>, sc_dt::sc_uint<43>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
2b 54 24 10          	sub    0x10(%esp),%edx
1b 4c 24 14          	sbb    0x14(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 ca                	mov    %ecx,%edx
81 e2 ff 0f 00 00    	and    $0xfff,%edx
89 50 04             	mov    %edx,0x4(%eax)
c2 04 00             	ret    $0x4

<sub_s64_s64_s64(sc_dt::sc_int<64>, sc_dt::sc_int<64>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
2b 54 24 10          	sub    0x10(%esp),%edx
1b 4c 24 14          	sbb    0x14(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<sub_u64_u64_u64(sc_dt::sc_uint<64>, sc_dt::sc_uint<64>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
2b 54 24 10          	sub    0x10(%esp),%edx
1b 4c 24 14          	sbb    0x14(%esp),%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
c2 04 00             	ret    $0x4

<sub_s13_s13_1_s13(sc_dt::sc_int<13>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
83 ea 01             	sub    $0x1,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<sub_s13_s13_1_s14(sc_dt::sc_int<13>, sc_dt::sc_int<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
83 ea 01             	sub    $0x1,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<sub_s13_s13_b_s13(sc_dt::sc_int<13>, sc_dt::sc_int<13>, bool)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 10       	movzbl 0x10(%esp),%ecx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
29 ca                	sub    %ecx,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<sub_s13_s13_b_s14(sc_dt::sc_int<13>, sc_dt::sc_int<13>, bool)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 10       	movzbl 0x10(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
29 ca                	sub    %ecx,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<sub_s13_s13_u1_s13(sc_dt::sc_int<13>, sc_dt::sc_int<13>, sc_dt::sc_uint<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
2b 54 24 10          	sub    0x10(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_s13_s13_u1_s14(sc_dt::sc_int<13>, sc_dt::sc_int<13>, sc_dt::sc_uint<1>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
2b 54 24 10          	sub    0x10(%esp),%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_u13_1_u13(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
83 ea 01             	sub    $0x1,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<sub_u13_u13_1_u14(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
83 ea 01             	sub    $0x1,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<sub_u13_u13_b_u13(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>, bool)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 10       	movzbl 0x10(%esp),%ecx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
29 ca                	sub    %ecx,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<sub_u13_u13_b_u14(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>, bool)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 10       	movzbl 0x10(%esp),%ecx
2b 54 24 0c          	sub    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
29 ca                	sub    %ecx,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<sub_u13_u13_u1_u13(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>, sc_dt::sc_uint<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
2b 54 24 10          	sub    0x10(%esp),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<sub_u13_u13_u1_u14(sc_dt::sc_uint<13>, sc_dt::sc_uint<13>, sc_dt::sc_uint<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
2b 54 24 0c          	sub    0xc(%esp),%edx
2b 54 24 10          	sub    0x10(%esp),%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<inc_post_s13(sc_dt::sc_int<13>&)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 19                	mov    (%ecx),%ebx
8d 53 01             	lea    0x1(%ebx),%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 11                	mov    %edx,(%ecx)
89 18                	mov    %ebx,(%eax)
5b                   	pop    %ebx
c2 04 00             	ret    $0x4

<inc_pre_s13(sc_dt::sc_int<13>&)>:
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
8b 11                	mov    (%ecx),%edx
83 c2 01             	add    $0x1,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 11                	mov    %edx,(%ecx)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<inc_post_u13(sc_dt::sc_uint<13>&)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 19                	mov    (%ecx),%ebx
8d 53 01             	lea    0x1(%ebx),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 11                	mov    %edx,(%ecx)
89 18                	mov    %ebx,(%eax)
5b                   	pop    %ebx
c2 04 00             	ret    $0x4

<inc_pre_u13(sc_dt::sc_uint<13>&)>:
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
8b 11                	mov    (%ecx),%edx
83 c2 01             	add    $0x1,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 11                	mov    %edx,(%ecx)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<dec_post_s13(sc_dt::sc_int<13>&)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 19                	mov    (%ecx),%ebx
8d 53 ff             	lea    -0x1(%ebx),%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 11                	mov    %edx,(%ecx)
89 18                	mov    %ebx,(%eax)
5b                   	pop    %ebx
c2 04 00             	ret    $0x4

<dec_pre_s13(sc_dt::sc_int<13>&)>:
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
8b 11                	mov    (%ecx),%edx
83 ea 01             	sub    $0x1,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 11                	mov    %edx,(%ecx)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<dec_post_u13(sc_dt::sc_uint<13>&)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 19                	mov    (%ecx),%ebx
8d 53 ff             	lea    -0x1(%ebx),%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 11                	mov    %edx,(%ecx)
89 18                	mov    %ebx,(%eax)
5b                   	pop    %ebx
c2 04 00             	ret    $0x4

<dec_pre_u13(sc_dt::sc_uint<13>&)>:
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
8b 11                	mov    (%ecx),%edx
83 ea 01             	sub    $0x1,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 11                	mov    %edx,(%ecx)
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_u11_2_u14(sc_dt::sc_uint<11>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 02             	shl    $0x2,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_u11_3_u14(sc_dt::sc_uint<11>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 03             	shl    $0x3,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_u11_4_u14(sc_dt::sc_uint<11>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 04             	shl    $0x4,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_u11_u1_u14(sc_dt::sc_uint<11>, sc_dt::sc_uint<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_u11_u2_u14(sc_dt::sc_uint<11>, sc_dt::sc_uint<2>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_u11_u3_u14(sc_dt::sc_uint<11>, sc_dt::sc_uint<3>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_u11_s1_u14(sc_dt::sc_uint<11>, sc_dt::sc_int<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_u11_s2_u14(sc_dt::sc_uint<11>, sc_dt::sc_int<2>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_u11_s3_u14(sc_dt::sc_uint<11>, sc_dt::sc_int<3>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_u11_s4_u14(sc_dt::sc_uint<11>, sc_dt::sc_int<4>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_u11_b_u14(sc_dt::sc_uint<11>, bool)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_u11_sc_u14(sc_dt::sc_uint<11>, signed char)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f be 4c 24 0c       	movsbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_u11_uc_u14(sc_dt::sc_uint<11>, unsigned char)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_u11_ss_u14(sc_dt::sc_uint<11>, short)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f bf 4c 24 0c       	movswl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_u11_us_u14(sc_dt::sc_uint<11>, unsigned short)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b7 4c 24 0c       	movzwl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_u11_si_u14(sc_dt::sc_uint<11>, int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_u11_ui_u14(sc_dt::sc_uint<11>, unsigned int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_u11_sl_u14(sc_dt::sc_uint<11>, long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_u11_ul_u14(sc_dt::sc_uint<11>, unsigned long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
81 e2 ff 3f 00 00    	and    $0x3fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_s11_2_s14(sc_dt::sc_int<11>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 02             	shl    $0x2,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_s11_3_s14(sc_dt::sc_int<11>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 03             	shl    $0x3,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_s11_4_s14(sc_dt::sc_int<11>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 16             	shl    $0x16,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_s11_u1_s14(sc_dt::sc_int<11>, sc_dt::sc_uint<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_s11_u2_s14(sc_dt::sc_int<11>, sc_dt::sc_uint<2>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_s11_u3_s14(sc_dt::sc_int<11>, sc_dt::sc_uint<3>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_s11_s1_s14(sc_dt::sc_int<11>, sc_dt::sc_int<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_s11_s2_s14(sc_dt::sc_int<11>, sc_dt::sc_int<2>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_s11_s3_s14(sc_dt::sc_int<11>, sc_dt::sc_int<3>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_s11_s4_s14(sc_dt::sc_int<11>, sc_dt::sc_int<4>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_s11_b_s14(sc_dt::sc_int<11>, bool)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_s11_sc_s14(sc_dt::sc_int<11>, signed char)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f be 4c 24 0c       	movsbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_s11_uc_s14(sc_dt::sc_int<11>, unsigned char)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_s11_ss_s14(sc_dt::sc_int<11>, short)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f bf 4c 24 0c       	movswl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_s11_us_s14(sc_dt::sc_int<11>, unsigned short)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b7 4c 24 0c       	movzwl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_s11_si_s14(sc_dt::sc_int<11>, int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_s11_ui_s14(sc_dt::sc_int<11>, unsigned int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<lsh_s11_sl_s14(sc_dt::sc_int<11>, long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<lsh_s11_ul_s14(sc_dt::sc_int<11>, unsigned long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
c1 e2 12             	shl    $0x12,%edx
c1 fa 12             	sar    $0x12,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_2_u11(sc_dt::sc_uint<14>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 ea 02             	shr    $0x2,%edx
81 e2 ff 07 00 00    	and    $0x7ff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_3_u11(sc_dt::sc_uint<14>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 ea 03             	shr    $0x3,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_4_u11(sc_dt::sc_uint<14>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 ea 04             	shr    $0x4,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_u1_u14(sc_dt::sc_uint<14>, sc_dt::sc_uint<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_u2_u14(sc_dt::sc_uint<14>, sc_dt::sc_uint<2>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_u3_u14(sc_dt::sc_uint<14>, sc_dt::sc_uint<3>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_s1_u14(sc_dt::sc_uint<14>, sc_dt::sc_int<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_s2_u14(sc_dt::sc_uint<14>, sc_dt::sc_int<2>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_s3_u14(sc_dt::sc_uint<14>, sc_dt::sc_int<3>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_s4_u14(sc_dt::sc_uint<14>, sc_dt::sc_int<4>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_b_u14(sc_dt::sc_uint<14>, bool)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_sc_u14(sc_dt::sc_uint<14>, signed char)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f be 4c 24 0c       	movsbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_uc_u14(sc_dt::sc_uint<14>, unsigned char)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_ss_u14(sc_dt::sc_uint<14>, short)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f bf 4c 24 0c       	movswl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_us_u14(sc_dt::sc_uint<14>, unsigned short)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b7 4c 24 0c       	movzwl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_si_u14(sc_dt::sc_uint<14>, int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_ui_u14(sc_dt::sc_uint<14>, unsigned int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_sl_u14(sc_dt::sc_uint<14>, long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_ul_u14(sc_dt::sc_uint<14>, unsigned long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_u1_u13(sc_dt::sc_uint<14>, sc_dt::sc_uint<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_u2_u13(sc_dt::sc_uint<14>, sc_dt::sc_uint<2>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_u3_u13(sc_dt::sc_uint<14>, sc_dt::sc_uint<3>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_s1_u13(sc_dt::sc_uint<14>, sc_dt::sc_int<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_s2_u13(sc_dt::sc_uint<14>, sc_dt::sc_int<2>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_s3_u13(sc_dt::sc_uint<14>, sc_dt::sc_int<3>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_s4_u13(sc_dt::sc_uint<14>, sc_dt::sc_int<4>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_b_u13(sc_dt::sc_uint<14>, bool)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_sc_u13(sc_dt::sc_uint<14>, signed char)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f be 4c 24 0c       	movsbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_uc_u13(sc_dt::sc_uint<14>, unsigned char)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_ss_u13(sc_dt::sc_uint<14>, short)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f bf 4c 24 0c       	movswl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_us_u13(sc_dt::sc_uint<14>, unsigned short)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b7 4c 24 0c       	movzwl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_si_u13(sc_dt::sc_uint<14>, int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_ui_u13(sc_dt::sc_uint<14>, unsigned int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_u14_sl_u13(sc_dt::sc_uint<14>, long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_u14_ul_u13(sc_dt::sc_uint<14>, unsigned long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
81 e2 ff 1f 00 00    	and    $0x1fff,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_2_s11(sc_dt::sc_int<14>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e2 13             	shl    $0x13,%edx
c1 fa 15             	sar    $0x15,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_3_s11(sc_dt::sc_int<14>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 fa 03             	sar    $0x3,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_4_s11(sc_dt::sc_int<14>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 fa 04             	sar    $0x4,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_u1_s14(sc_dt::sc_int<14>, sc_dt::sc_uint<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_u2_s14(sc_dt::sc_int<14>, sc_dt::sc_uint<2>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_u3_s14(sc_dt::sc_int<14>, sc_dt::sc_uint<3>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_s1_s14(sc_dt::sc_int<14>, sc_dt::sc_int<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_s2_s14(sc_dt::sc_int<14>, sc_dt::sc_int<2>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_s3_s14(sc_dt::sc_int<14>, sc_dt::sc_int<3>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_s4_s14(sc_dt::sc_int<14>, sc_dt::sc_int<4>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_b_s14(sc_dt::sc_int<14>, bool)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_sc_s14(sc_dt::sc_int<14>, signed char)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f be 4c 24 0c       	movsbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_uc_s14(sc_dt::sc_int<14>, unsigned char)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_ss_s14(sc_dt::sc_int<14>, short)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f bf 4c 24 0c       	movswl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_us_s14(sc_dt::sc_int<14>, unsigned short)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b7 4c 24 0c       	movzwl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_si_s14(sc_dt::sc_int<14>, int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_ui_s14(sc_dt::sc_int<14>, unsigned int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_sl_s14(sc_dt::sc_int<14>, long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_ul_s14(sc_dt::sc_int<14>, unsigned long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_u1_s13(sc_dt::sc_int<14>, sc_dt::sc_uint<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_u2_s13(sc_dt::sc_int<14>, sc_dt::sc_uint<2>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_u3_s13(sc_dt::sc_int<14>, sc_dt::sc_uint<3>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_s1_s13(sc_dt::sc_int<14>, sc_dt::sc_int<1>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_s2_s13(sc_dt::sc_int<14>, sc_dt::sc_int<2>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_s3_s13(sc_dt::sc_int<14>, sc_dt::sc_int<3>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_s4_s13(sc_dt::sc_int<14>, sc_dt::sc_int<4>)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_b_s13(sc_dt::sc_int<14>, bool)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_sc_s13(sc_dt::sc_int<14>, signed char)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f be 4c 24 0c       	movsbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_uc_s13(sc_dt::sc_int<14>, unsigned char)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_ss_s13(sc_dt::sc_int<14>, short)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f bf 4c 24 0c       	movswl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_us_s13(sc_dt::sc_int<14>, unsigned short)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b7 4c 24 0c       	movzwl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_si_s13(sc_dt::sc_int<14>, int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_ui_s13(sc_dt::sc_int<14>, unsigned int)>:
8b 54 24 08          	mov    0x8(%esp),%edx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4
90                   	nop    

<rsh_s14_sl_s13(sc_dt::sc_int<14>, long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<rsh_s14_ul_s13(sc_dt::sc_int<14>, unsigned long long)>:
8b 54 24 08          	mov    0x8(%esp),%edx
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 fa                	sar    %cl,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c2 04 00             	ret    $0x4

<bitref_s13c_5_b(sc_dt::sc_int<13> const&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitref_s13_5_b(sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitref_s13v_5_b(sc_dt::sc_int<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitref_s13c_5_si(sc_dt::sc_int<13> const&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitref_s13_5_si(sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitref_s13v_5_si(sc_dt::sc_int<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitref_s13c_si_b(sc_dt::sc_int<13> const&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitref_s13_si_b(sc_dt::sc_int<13>&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitref_s13v_si_b(sc_dt::sc_int<13>, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitref_u13c_5_b(sc_dt::sc_uint<13> const&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 e8 05             	shr    $0x5,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitref_u13_5_b(sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 e8 05             	shr    $0x5,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitref_u13v_5_b(sc_dt::sc_uint<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e8 05             	shr    $0x5,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitref_u13c_5_si(sc_dt::sc_uint<13> const&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 e8 05             	shr    $0x5,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitref_u13_5_si(sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 e8 05             	shr    $0x5,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitref_u13v_5_si(sc_dt::sc_uint<13>)>:
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e8 05             	shr    $0x5,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitref_u13c_si_b(sc_dt::sc_uint<13> const&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 e8                	shr    %cl,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitref_u13_si_b(sc_dt::sc_uint<13>&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 e8                	shr    %cl,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitref_u13v_si_b(sc_dt::sc_uint<13>, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
d3 e8                	shr    %cl,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitref_s43c_5_b(sc_dt::sc_int<43> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 58 04             	mov    0x4(%eax),%ebx
8b 08                	mov    (%eax),%ecx
0f ac d9 05          	shrd   $0x5,%ebx,%ecx
89 c8                	mov    %ecx,%eax
c1 fb 05             	sar    $0x5,%ebx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitref_s43_5_b(sc_dt::sc_int<43>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 58 04             	mov    0x4(%eax),%ebx
8b 08                	mov    (%eax),%ecx
0f ac d9 05          	shrd   $0x5,%ebx,%ecx
89 c8                	mov    %ecx,%eax
c1 fb 05             	sar    $0x5,%ebx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitref_s43v_5_b(sc_dt::sc_int<43>)>:
53                   	push   %ebx
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
5b                   	pop    %ebx
0f ac d0 05          	shrd   $0x5,%edx,%eax
89 c1                	mov    %eax,%ecx
83 e1 01             	and    $0x1,%ecx
0f b6 c1             	movzbl %cl,%eax
c1 fa 05             	sar    $0x5,%edx
c3                   	ret    

<bitref_s43c_5_si(sc_dt::sc_int<43> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 58 04             	mov    0x4(%eax),%ebx
8b 08                	mov    (%eax),%ecx
0f ac d9 05          	shrd   $0x5,%ebx,%ecx
89 c8                	mov    %ecx,%eax
c1 fb 05             	sar    $0x5,%ebx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitref_s43_5_si(sc_dt::sc_int<43>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 58 04             	mov    0x4(%eax),%ebx
8b 08                	mov    (%eax),%ecx
0f ac d9 05          	shrd   $0x5,%ebx,%ecx
89 c8                	mov    %ecx,%eax
c1 fb 05             	sar    $0x5,%ebx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitref_s43v_5_si(sc_dt::sc_int<43>)>:
53                   	push   %ebx
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
5b                   	pop    %ebx
0f ac d0 05          	shrd   $0x5,%edx,%eax
89 c1                	mov    %eax,%ecx
83 e1 01             	and    $0x1,%ecx
0f b6 c1             	movzbl %cl,%eax
c1 fa 05             	sar    $0x5,%edx
c3                   	ret    

<bitref_s43c_si_b(sc_dt::sc_int<43> const&, int)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 10          	mov    0x10(%esp),%eax
0f b6 4c 24 14       	movzbl 0x14(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
89 74 24 04          	mov    %esi,0x4(%esp)
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 78 04             	mov    0x4(%eax),%edi
8b 30                	mov    (%eax),%esi
0f ad fe             	shrd   %cl,%edi,%esi
d3 ff                	sar    %cl,%edi
f6 c1 20             	test   $0x20,%cl
74 05                	je     337f <bitref_s43c_si_b(sc_dt::sc_int<43> const&, int)+0x2b>
89 fe                	mov    %edi,%esi
c1 ff 1f             	sar    $0x1f,%edi
89 f0                	mov    %esi,%eax
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 e0 01             	and    $0x1,%eax
8b 7c 24 08          	mov    0x8(%esp),%edi
0f b6 c0             	movzbl %al,%eax
83 c4 0c             	add    $0xc,%esp
c3                   	ret    

<bitref_s43_si_b(sc_dt::sc_int<43>&, int)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 10          	mov    0x10(%esp),%eax
0f b6 4c 24 14       	movzbl 0x14(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
89 74 24 04          	mov    %esi,0x4(%esp)
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 78 04             	mov    0x4(%eax),%edi
8b 30                	mov    (%eax),%esi
0f ad fe             	shrd   %cl,%edi,%esi
d3 ff                	sar    %cl,%edi
f6 c1 20             	test   $0x20,%cl
74 05                	je     33c1 <bitref_s43_si_b(sc_dt::sc_int<43>&, int)+0x2b>
89 fe                	mov    %edi,%esi
c1 ff 1f             	sar    $0x1f,%edi
89 f0                	mov    %esi,%eax
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 e0 01             	and    $0x1,%eax
8b 7c 24 08          	mov    0x8(%esp),%edi
0f b6 c0             	movzbl %al,%eax
83 c4 0c             	add    $0xc,%esp
c3                   	ret    

<bitref_s43v_si_b(sc_dt::sc_int<43>, int)>:
83 ec 0c             	sub    $0xc,%esp
0f b6 4c 24 18       	movzbl 0x18(%esp),%ecx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 10          	mov    0x10(%esp),%esi
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 7c 24 14          	mov    0x14(%esp),%edi
89 1c 24             	mov    %ebx,(%esp)
0f ad fe             	shrd   %cl,%edi,%esi
d3 ff                	sar    %cl,%edi
f6 c1 20             	test   $0x20,%cl
74 05                	je     3402 <bitref_s43v_si_b(sc_dt::sc_int<43>, int)+0x2a>
89 fe                	mov    %edi,%esi
c1 ff 1f             	sar    $0x1f,%edi
89 f0                	mov    %esi,%eax
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 e0 01             	and    $0x1,%eax
8b 7c 24 08          	mov    0x8(%esp),%edi
0f b6 c0             	movzbl %al,%eax
83 c4 0c             	add    $0xc,%esp
c3                   	ret    
90                   	nop    

<bitref_u43c_5_b(sc_dt::sc_uint<43> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 58 04             	mov    0x4(%eax),%ebx
8b 08                	mov    (%eax),%ecx
0f ac d9 05          	shrd   $0x5,%ebx,%ecx
89 c8                	mov    %ecx,%eax
c1 eb 05             	shr    $0x5,%ebx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitref_u43_5_b(sc_dt::sc_uint<43>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 58 04             	mov    0x4(%eax),%ebx
8b 08                	mov    (%eax),%ecx
0f ac d9 05          	shrd   $0x5,%ebx,%ecx
89 c8                	mov    %ecx,%eax
c1 eb 05             	shr    $0x5,%ebx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitref_u43v_5_b(sc_dt::sc_uint<43>)>:
53                   	push   %ebx
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
5b                   	pop    %ebx
0f ac d0 05          	shrd   $0x5,%edx,%eax
89 c1                	mov    %eax,%ecx
83 e1 01             	and    $0x1,%ecx
0f b6 c1             	movzbl %cl,%eax
c1 ea 05             	shr    $0x5,%edx
c3                   	ret    

<bitref_u43c_5_si(sc_dt::sc_uint<43> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 58 04             	mov    0x4(%eax),%ebx
8b 08                	mov    (%eax),%ecx
0f ac d9 05          	shrd   $0x5,%ebx,%ecx
89 c8                	mov    %ecx,%eax
c1 eb 05             	shr    $0x5,%ebx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitref_u43_5_si(sc_dt::sc_uint<43>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 58 04             	mov    0x4(%eax),%ebx
8b 08                	mov    (%eax),%ecx
0f ac d9 05          	shrd   $0x5,%ebx,%ecx
89 c8                	mov    %ecx,%eax
c1 eb 05             	shr    $0x5,%ebx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitref_u43v_5_si(sc_dt::sc_uint<43>)>:
53                   	push   %ebx
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
5b                   	pop    %ebx
0f ac d0 05          	shrd   $0x5,%edx,%eax
89 c1                	mov    %eax,%ecx
83 e1 01             	and    $0x1,%ecx
0f b6 c1             	movzbl %cl,%eax
c1 ea 05             	shr    $0x5,%edx
c3                   	ret    

<bitref_u43c_si_b(sc_dt::sc_uint<43> const&, int)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 10          	mov    0x10(%esp),%eax
0f b6 4c 24 14       	movzbl 0x14(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
89 74 24 04          	mov    %esi,0x4(%esp)
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 78 04             	mov    0x4(%eax),%edi
8b 30                	mov    (%eax),%esi
0f ad fe             	shrd   %cl,%edi,%esi
d3 ef                	shr    %cl,%edi
f6 c1 20             	test   $0x20,%cl
74 04                	je     34e8 <bitref_u43c_si_b(sc_dt::sc_uint<43> const&, int)+0x2a>
89 fe                	mov    %edi,%esi
31 ff                	xor    %edi,%edi
89 f0                	mov    %esi,%eax
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 e0 01             	and    $0x1,%eax
8b 7c 24 08          	mov    0x8(%esp),%edi
0f b6 c0             	movzbl %al,%eax
83 c4 0c             	add    $0xc,%esp
c3                   	ret    
90                   	nop    

<bitref_u43_si_b(sc_dt::sc_uint<43>&, int)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 10          	mov    0x10(%esp),%eax
0f b6 4c 24 14       	movzbl 0x14(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
89 74 24 04          	mov    %esi,0x4(%esp)
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 78 04             	mov    0x4(%eax),%edi
8b 30                	mov    (%eax),%esi
0f ad fe             	shrd   %cl,%edi,%esi
d3 ef                	shr    %cl,%edi
f6 c1 20             	test   $0x20,%cl
74 04                	je     352a <bitref_u43_si_b(sc_dt::sc_uint<43>&, int)+0x2a>
89 fe                	mov    %edi,%esi
31 ff                	xor    %edi,%edi
89 f0                	mov    %esi,%eax
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 e0 01             	and    $0x1,%eax
8b 7c 24 08          	mov    0x8(%esp),%edi
0f b6 c0             	movzbl %al,%eax
83 c4 0c             	add    $0xc,%esp
c3                   	ret    
90                   	nop    

<bitref_u43v_si_b(sc_dt::sc_uint<43>, int)>:
83 ec 0c             	sub    $0xc,%esp
0f b6 4c 24 18       	movzbl 0x18(%esp),%ecx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 10          	mov    0x10(%esp),%esi
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 7c 24 14          	mov    0x14(%esp),%edi
89 1c 24             	mov    %ebx,(%esp)
0f ad fe             	shrd   %cl,%edi,%esi
d3 ef                	shr    %cl,%edi
f6 c1 20             	test   $0x20,%cl
74 04                	je     356b <bitref_u43v_si_b(sc_dt::sc_uint<43>, int)+0x29>
89 fe                	mov    %edi,%esi
31 ff                	xor    %edi,%edi
89 f0                	mov    %esi,%eax
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 e0 01             	and    $0x1,%eax
8b 7c 24 08          	mov    0x8(%esp),%edi
0f b6 c0             	movzbl %al,%eax
83 c4 0c             	add    $0xc,%esp
c3                   	ret    

<bitand_s13c_5_b(sc_dt::sc_int<13> const&, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
22 44 24 08          	and    0x8(%esp),%al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitand_s13_5_b(sc_dt::sc_int<13>&, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
22 44 24 08          	and    0x8(%esp),%al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitand_s13c_si_b(sc_dt::sc_int<13> const&, int, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
22 44 24 0c          	and    0xc(%esp),%al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitand_s13_si_b(sc_dt::sc_int<13>&, int, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
22 44 24 0c          	and    0xc(%esp),%al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitand_b_s13c_5(bool, sc_dt::sc_int<13> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
22 44 24 04          	and    0x4(%esp),%al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitand_b_s13_5(bool, sc_dt::sc_int<13>&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
22 44 24 04          	and    0x4(%esp),%al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitand_b_s13c_si(bool, sc_dt::sc_int<13> const&, int)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
22 44 24 04          	and    0x4(%esp),%al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitand_b_s13_si(bool, sc_dt::sc_int<13>&, int)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
22 44 24 04          	and    0x4(%esp),%al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitand_s13c_5_u11c_7(sc_dt::sc_int<13> const&, sc_dt::sc_uint<11> const&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 12                	mov    (%edx),%edx
8b 00                	mov    (%eax),%eax
c1 fa 05             	sar    $0x5,%edx
c1 e8 07             	shr    $0x7,%eax
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    

<bitand_s13_5_u11c_7(sc_dt::sc_int<13>&, sc_dt::sc_uint<11> const&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 12                	mov    (%edx),%edx
8b 00                	mov    (%eax),%eax
c1 fa 05             	sar    $0x5,%edx
c1 e8 07             	shr    $0x7,%eax
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    

<bitand_s13c_si_u11c_7(sc_dt::sc_int<13> const&, int, sc_dt::sc_uint<11> const&)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 12                	mov    (%edx),%edx
8b 00                	mov    (%eax),%eax
c1 ea 07             	shr    $0x7,%edx
d3 f8                	sar    %cl,%eax
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    
90                   	nop    

<bitand_s13_si_u11c_7(sc_dt::sc_int<13>&, int, sc_dt::sc_uint<11> const&)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 12                	mov    (%edx),%edx
8b 00                	mov    (%eax),%eax
c1 ea 07             	shr    $0x7,%edx
d3 f8                	sar    %cl,%eax
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    
90                   	nop    

<bitand_s13c_5_u11_7(sc_dt::sc_int<13> const&, sc_dt::sc_uint<11>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 12                	mov    (%edx),%edx
8b 00                	mov    (%eax),%eax
c1 fa 05             	sar    $0x5,%edx
c1 e8 07             	shr    $0x7,%eax
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    

<bitand_s13_5_u11_7(sc_dt::sc_int<13>&, sc_dt::sc_uint<11>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 12                	mov    (%edx),%edx
8b 00                	mov    (%eax),%eax
c1 fa 05             	sar    $0x5,%edx
c1 e8 07             	shr    $0x7,%eax
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    

<bitand_s13c_si_u11_7(sc_dt::sc_int<13> const&, int, sc_dt::sc_uint<11>&)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 12                	mov    (%edx),%edx
8b 00                	mov    (%eax),%eax
c1 ea 07             	shr    $0x7,%edx
d3 f8                	sar    %cl,%eax
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    
90                   	nop    

<bitand_s13_si_u11_7(sc_dt::sc_int<13>&, int, sc_dt::sc_uint<11>&)>:
8b 54 24 0c          	mov    0xc(%esp),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 12                	mov    (%edx),%edx
8b 00                	mov    (%eax),%eax
c1 ea 07             	shr    $0x7,%edx
d3 f8                	sar    %cl,%eax
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    
90                   	nop    

<bitand_s13c_5_u11c_si(sc_dt::sc_int<13> const&, sc_dt::sc_uint<11> const&, int)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 12                	mov    (%edx),%edx
8b 00                	mov    (%eax),%eax
c1 fa 05             	sar    $0x5,%edx
d3 e8                	shr    %cl,%eax
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    
90                   	nop    

<bitand_s13_5_u11c_si(sc_dt::sc_int<13>&, sc_dt::sc_uint<11> const&, int)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 12                	mov    (%edx),%edx
8b 00                	mov    (%eax),%eax
c1 fa 05             	sar    $0x5,%edx
d3 e8                	shr    %cl,%eax
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    
90                   	nop    

<bitand_s13c_si_u11c_si(sc_dt::sc_int<13> const&, int, sc_dt::sc_uint<11> const&, int)>:
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 54 24 04          	mov    0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
8b 12                	mov    (%edx),%edx
d3 e8                	shr    %cl,%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
d3 fa                	sar    %cl,%edx
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    

<bitand_s13_si_u11c_si(sc_dt::sc_int<13>&, int, sc_dt::sc_uint<11> const&, int)>:
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 54 24 04          	mov    0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
8b 12                	mov    (%edx),%edx
d3 e8                	shr    %cl,%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
d3 fa                	sar    %cl,%edx
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    

<bitand_s13c_5_u11_si(sc_dt::sc_int<13> const&, sc_dt::sc_uint<11>&, int)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 12                	mov    (%edx),%edx
8b 00                	mov    (%eax),%eax
c1 fa 05             	sar    $0x5,%edx
d3 e8                	shr    %cl,%eax
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    
90                   	nop    

<bitand_s13_5_u11_si(sc_dt::sc_int<13>&, sc_dt::sc_uint<11>&, int)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 12                	mov    (%edx),%edx
8b 00                	mov    (%eax),%eax
c1 fa 05             	sar    $0x5,%edx
d3 e8                	shr    %cl,%eax
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    
90                   	nop    

<bitand_s13c_si_u11_si(sc_dt::sc_int<13> const&, int, sc_dt::sc_uint<11>&, int)>:
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 54 24 04          	mov    0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
8b 12                	mov    (%edx),%edx
d3 e8                	shr    %cl,%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
d3 fa                	sar    %cl,%edx
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    

<bitand_s13_si_u11_si(sc_dt::sc_int<13>&, int, sc_dt::sc_uint<11>&, int)>:
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 54 24 04          	mov    0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
8b 12                	mov    (%edx),%edx
d3 e8                	shr    %cl,%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
d3 fa                	sar    %cl,%edx
83 e2 01             	and    $0x1,%edx
21 d0                	and    %edx,%eax
c3                   	ret    

<bitor_s13c_5_b(sc_dt::sc_int<13> const&, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
09 c2                	or     %eax,%edx
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitor_s13_5_b(sc_dt::sc_int<13>&, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
09 c2                	or     %eax,%edx
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitor_s13c_si_b(sc_dt::sc_int<13> const&, int, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
0f b6 54 24 0c       	movzbl 0xc(%esp),%edx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
09 d0                	or     %edx,%eax
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitor_s13_si_b(sc_dt::sc_int<13>&, int, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
0f b6 54 24 0c       	movzbl 0xc(%esp),%edx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
09 d0                	or     %edx,%eax
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitor_b_s13c_5(bool, sc_dt::sc_int<13> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
0f b6 54 24 04       	movzbl 0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
09 d0                	or     %edx,%eax
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitor_b_s13_5(bool, sc_dt::sc_int<13>&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
0f b6 54 24 04       	movzbl 0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
09 d0                	or     %edx,%eax
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitor_b_s13c_si(bool, sc_dt::sc_int<13> const&, int)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
0f b6 54 24 04       	movzbl 0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
09 d0                	or     %edx,%eax
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitor_b_s13_si(bool, sc_dt::sc_int<13>&, int)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
0f b6 54 24 04       	movzbl 0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
09 d0                	or     %edx,%eax
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitor_s13c_5_u11c_7(sc_dt::sc_int<13> const&, sc_dt::sc_uint<11> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 ea 07             	shr    $0x7,%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitor_s13_5_u11c_7(sc_dt::sc_int<13>&, sc_dt::sc_uint<11> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 ea 07             	shr    $0x7,%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitor_s13c_si_u11c_7(sc_dt::sc_int<13> const&, int, sc_dt::sc_uint<11> const&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
d3 fa                	sar    %cl,%edx
8b 00                	mov    (%eax),%eax
c1 e8 07             	shr    $0x7,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitor_s13_si_u11c_7(sc_dt::sc_int<13>&, int, sc_dt::sc_uint<11> const&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
d3 fa                	sar    %cl,%edx
8b 00                	mov    (%eax),%eax
c1 e8 07             	shr    $0x7,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitor_s13c_5_u11_7(sc_dt::sc_int<13> const&, sc_dt::sc_uint<11>&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 ea 07             	shr    $0x7,%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitor_s13_5_u11_7(sc_dt::sc_int<13>&, sc_dt::sc_uint<11>&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
c1 ea 07             	shr    $0x7,%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitor_s13c_si_u11_7(sc_dt::sc_int<13> const&, int, sc_dt::sc_uint<11>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
d3 fa                	sar    %cl,%edx
8b 00                	mov    (%eax),%eax
c1 e8 07             	shr    $0x7,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitor_s13_si_u11_7(sc_dt::sc_int<13>&, int, sc_dt::sc_uint<11>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
d3 fa                	sar    %cl,%edx
8b 00                	mov    (%eax),%eax
c1 e8 07             	shr    $0x7,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitor_s13c_5_u11c_si(sc_dt::sc_int<13> const&, sc_dt::sc_uint<11> const&, int)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitor_s13_5_u11c_si(sc_dt::sc_int<13>&, sc_dt::sc_uint<11> const&, int)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitor_s13c_si_u11c_si(sc_dt::sc_int<13> const&, int, sc_dt::sc_uint<11> const&, int)>:
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitor_s13_si_u11c_si(sc_dt::sc_int<13>&, int, sc_dt::sc_uint<11> const&, int)>:
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitor_s13c_5_u11_si(sc_dt::sc_int<13> const&, sc_dt::sc_uint<11>&, int)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitor_s13_5_u11_si(sc_dt::sc_int<13>&, sc_dt::sc_uint<11>&, int)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    
90                   	nop    

<bitor_s13c_si_u11_si(sc_dt::sc_int<13> const&, int, sc_dt::sc_uint<11>&, int)>:
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitor_s13_si_u11_si(sc_dt::sc_int<13>&, int, sc_dt::sc_uint<11>&, int)>:
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 ea                	shr    %cl,%edx
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c3                   	ret    

<bitxor_s13c_5_b(sc_dt::sc_int<13> const&, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
38 44 24 08          	cmp    %al,0x8(%esp)
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitxor_s13_5_b(sc_dt::sc_int<13>&, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
38 44 24 08          	cmp    %al,0x8(%esp)
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitxor_s13c_si_b(sc_dt::sc_int<13> const&, int, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
38 44 24 0c          	cmp    %al,0xc(%esp)
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitxor_s13_si_b(sc_dt::sc_int<13>&, int, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
38 44 24 0c          	cmp    %al,0xc(%esp)
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitxor_b_s13c_5(bool, sc_dt::sc_int<13> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
38 44 24 04          	cmp    %al,0x4(%esp)
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitxor_b_s13_5(bool, sc_dt::sc_int<13>&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
38 44 24 04          	cmp    %al,0x4(%esp)
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitxor_b_s13c_si(bool, sc_dt::sc_int<13> const&, int)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
38 44 24 04          	cmp    %al,0x4(%esp)
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitxor_b_s13_si(bool, sc_dt::sc_int<13>&, int)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 00                	mov    (%eax),%eax
d3 f8                	sar    %cl,%eax
83 e0 01             	and    $0x1,%eax
38 44 24 04          	cmp    %al,0x4(%esp)
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitxor_s13c_5_u11c_7(sc_dt::sc_int<13> const&, sc_dt::sc_uint<11> const&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
c1 fa 05             	sar    $0x5,%edx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
c1 e8 07             	shr    $0x7,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitxor_s13_5_u11c_7(sc_dt::sc_int<13>&, sc_dt::sc_uint<11> const&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
c1 fa 05             	sar    $0x5,%edx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
c1 e8 07             	shr    $0x7,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitxor_s13c_si_u11c_7(sc_dt::sc_int<13> const&, int, sc_dt::sc_uint<11> const&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
d3 fa                	sar    %cl,%edx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
c1 e8 07             	shr    $0x7,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitxor_s13_si_u11c_7(sc_dt::sc_int<13>&, int, sc_dt::sc_uint<11> const&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
d3 fa                	sar    %cl,%edx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
c1 e8 07             	shr    $0x7,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitxor_s13c_5_u11_7(sc_dt::sc_int<13> const&, sc_dt::sc_uint<11>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
c1 fa 05             	sar    $0x5,%edx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
c1 e8 07             	shr    $0x7,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitxor_s13_5_u11_7(sc_dt::sc_int<13>&, sc_dt::sc_uint<11>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
c1 fa 05             	sar    $0x5,%edx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
c1 e8 07             	shr    $0x7,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitxor_s13c_si_u11_7(sc_dt::sc_int<13> const&, int, sc_dt::sc_uint<11>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
d3 fa                	sar    %cl,%edx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
c1 e8 07             	shr    $0x7,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitxor_s13_si_u11_7(sc_dt::sc_int<13>&, int, sc_dt::sc_uint<11>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
d3 fa                	sar    %cl,%edx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
c1 e8 07             	shr    $0x7,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitxor_s13c_5_u11c_si(sc_dt::sc_int<13> const&, sc_dt::sc_uint<11> const&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
c1 fa 05             	sar    $0x5,%edx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
d3 e8                	shr    %cl,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitxor_s13_5_u11c_si(sc_dt::sc_int<13>&, sc_dt::sc_uint<11> const&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
c1 fa 05             	sar    $0x5,%edx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
d3 e8                	shr    %cl,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitxor_s13c_si_u11c_si(sc_dt::sc_int<13> const&, int, sc_dt::sc_uint<11> const&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
d3 fa                	sar    %cl,%edx
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
d3 e8                	shr    %cl,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitxor_s13_si_u11c_si(sc_dt::sc_int<13>&, int, sc_dt::sc_uint<11> const&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
d3 fa                	sar    %cl,%edx
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
d3 e8                	shr    %cl,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitxor_s13c_5_u11_si(sc_dt::sc_int<13> const&, sc_dt::sc_uint<11>&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
c1 fa 05             	sar    $0x5,%edx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
d3 e8                	shr    %cl,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitxor_s13_5_u11_si(sc_dt::sc_int<13>&, sc_dt::sc_uint<11>&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 08          	mov    0x8(%esp),%eax
c1 fa 05             	sar    $0x5,%edx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
d3 e8                	shr    %cl,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    

<bitxor_s13c_si_u11_si(sc_dt::sc_int<13> const&, int, sc_dt::sc_uint<11>&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
d3 fa                	sar    %cl,%edx
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
d3 e8                	shr    %cl,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitxor_s13_si_u11_si(sc_dt::sc_int<13>&, int, sc_dt::sc_uint<11>&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
8b 10                	mov    (%eax),%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
d3 fa                	sar    %cl,%edx
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 00                	mov    (%eax),%eax
83 e2 01             	and    $0x1,%edx
d3 e8                	shr    %cl,%eax
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c3                   	ret    
90                   	nop    

<bitass_u1_s13c_5(sc_dt::sc_uint<1>&, sc_dt::sc_int<13> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 54 24 04          	mov    0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<bitass_u1_s13_5(sc_dt::sc_uint<1>&, sc_dt::sc_int<13>&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 54 24 04          	mov    0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<bitass_s13_5_0(sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 20 df             	andl   $0xffffffdf,(%eax)
c3                   	ret    

<bitass_s13_5_f(sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 20 df             	andl   $0xffffffdf,(%eax)
c3                   	ret    

<bitass_s13_5_1(sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 08 20             	orl    $0x20,(%eax)
c3                   	ret    

<bitass_s13_5_t(sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 08 20             	orl    $0x20,(%eax)
c3                   	ret    

<bitass_s13_5_b(sc_dt::sc_int<13>&, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 08                	mov    (%eax),%ecx
c1 e2 05             	shl    $0x5,%edx
83 e1 df             	and    $0xffffffdf,%ecx
09 ca                	or     %ecx,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    

<bitass_s13_12_0(sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
81 20 ff 0f 00 00    	andl   $0xfff,(%eax)
c3                   	ret    
90                   	nop    

<bitass_s13_12_f(sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
81 20 ff 0f 00 00    	andl   $0xfff,(%eax)
c3                   	ret    
90                   	nop    

<bitass_s13_12_1(sc_dt::sc_int<13>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 02                	mov    (%edx),%eax
80 cc 10             	or     $0x10,%ah
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    

<bitass_s13_12_t(sc_dt::sc_int<13>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 02                	mov    (%edx),%eax
80 cc 10             	or     $0x10,%ah
c1 e0 13             	shl    $0x13,%eax
c1 f8 13             	sar    $0x13,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    

<bitass_s13_12_b(sc_dt::sc_int<13>&, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 08                	mov    (%eax),%ecx
c1 e2 0c             	shl    $0xc,%edx
80 e5 ef             	and    $0xef,%ch
09 ca                	or     %ecx,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    

<bitass_s13_si_0(sc_dt::sc_int<13>&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
ba fe ff ff ff       	mov    $0xfffffffe,%edx
8b 4c 24 08          	mov    0x8(%esp),%ecx
d3 c2                	rol    %cl,%edx
23 10                	and    (%eax),%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    

<bitass_s13_si_f(sc_dt::sc_int<13>&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
ba fe ff ff ff       	mov    $0xfffffffe,%edx
8b 4c 24 08          	mov    0x8(%esp),%ecx
d3 c2                	rol    %cl,%edx
23 10                	and    (%eax),%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    

<bitass_s13_si_1(sc_dt::sc_int<13>&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
ba 01 00 00 00       	mov    $0x1,%edx
8b 4c 24 08          	mov    0x8(%esp),%ecx
d3 e2                	shl    %cl,%edx
0b 10                	or     (%eax),%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    

<bitass_s13_si_t(sc_dt::sc_int<13>&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
ba 01 00 00 00       	mov    $0x1,%edx
8b 4c 24 08          	mov    0x8(%esp),%ecx
d3 e2                	shl    %cl,%edx
0b 10                	or     (%eax),%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    

<bitass_s13_si_b(sc_dt::sc_int<13>&, int, bool)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
b8 fe ff ff ff       	mov    $0xfffffffe,%eax
8b 5c 24 08          	mov    0x8(%esp),%ebx
0f b6 54 24 10       	movzbl 0x10(%esp),%edx
d3 c0                	rol    %cl,%eax
23 03                	and    (%ebx),%eax
d3 e2                	shl    %cl,%edx
09 c2                	or     %eax,%edx
c1 e2 13             	shl    $0x13,%edx
c1 fa 13             	sar    $0x13,%edx
89 13                	mov    %edx,(%ebx)
5b                   	pop    %ebx
c3                   	ret    
90                   	nop    

<bitass_u13_5_0(sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 20 df             	andl   $0xffffffdf,(%eax)
c3                   	ret    

<bitass_u13_5_f(sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 20 df             	andl   $0xffffffdf,(%eax)
c3                   	ret    

<bitass_u13_5_1(sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 08 20             	orl    $0x20,(%eax)
c3                   	ret    

<bitass_u13_5_t(sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 08 20             	orl    $0x20,(%eax)
c3                   	ret    

<bitass_u13_5_b(sc_dt::sc_uint<13>&, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 08                	mov    (%eax),%ecx
c1 e2 05             	shl    $0x5,%edx
83 e1 df             	and    $0xffffffdf,%ecx
09 ca                	or     %ecx,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    

<bitass_u13_12_0(sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
81 20 ff ef ff ff    	andl   $0xffffefff,(%eax)
c3                   	ret    
90                   	nop    

<bitass_u13_12_f(sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
81 20 ff ef ff ff    	andl   $0xffffefff,(%eax)
c3                   	ret    
90                   	nop    

<bitass_u13_12_1(sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
81 08 00 10 00 00    	orl    $0x1000,(%eax)
c3                   	ret    
90                   	nop    

<bitass_u13_12_t(sc_dt::sc_uint<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
81 08 00 10 00 00    	orl    $0x1000,(%eax)
c3                   	ret    
90                   	nop    

<bitass_u13_12_b(sc_dt::sc_uint<13>&, bool)>:
8b 44 24 04          	mov    0x4(%esp),%eax
0f b6 54 24 08       	movzbl 0x8(%esp),%edx
8b 08                	mov    (%eax),%ecx
c1 e2 0c             	shl    $0xc,%edx
80 e5 ef             	and    $0xef,%ch
09 ca                	or     %ecx,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    

<bitass_u13_si_0(sc_dt::sc_uint<13>&, int)>:
8b 4c 24 08          	mov    0x8(%esp),%ecx
ba fe ff ff ff       	mov    $0xfffffffe,%edx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 c2                	rol    %cl,%edx
21 10                	and    %edx,(%eax)
c3                   	ret    

<bitass_u13_si_f(sc_dt::sc_uint<13>&, int)>:
8b 4c 24 08          	mov    0x8(%esp),%ecx
ba fe ff ff ff       	mov    $0xfffffffe,%edx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 c2                	rol    %cl,%edx
21 10                	and    %edx,(%eax)
c3                   	ret    

<bitass_u13_si_1(sc_dt::sc_uint<13>&, int)>:
8b 4c 24 08          	mov    0x8(%esp),%ecx
ba 01 00 00 00       	mov    $0x1,%edx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
09 10                	or     %edx,(%eax)
c3                   	ret    

<bitass_u13_si_t(sc_dt::sc_uint<13>&, int)>:
8b 4c 24 08          	mov    0x8(%esp),%ecx
ba 01 00 00 00       	mov    $0x1,%edx
8b 44 24 04          	mov    0x4(%esp),%eax
d3 e2                	shl    %cl,%edx
09 10                	or     %edx,(%eax)
c3                   	ret    

<bitass_u13_si_b(sc_dt::sc_uint<13>&, int, bool)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
b8 fe ff ff ff       	mov    $0xfffffffe,%eax
8b 5c 24 08          	mov    0x8(%esp),%ebx
0f b6 54 24 10       	movzbl 0x10(%esp),%edx
d3 c0                	rol    %cl,%eax
23 03                	and    (%ebx),%eax
d3 e2                	shl    %cl,%edx
09 c2                	or     %eax,%edx
89 13                	mov    %edx,(%ebx)
5b                   	pop    %ebx
c3                   	ret    
90                   	nop    

<bitass_s43_5_0(sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 20 df             	andl   $0xffffffdf,(%eax)
83 60 04 ff          	andl   $0xffffffff,0x4(%eax)
c3                   	ret    

<bitass_s43_5_f(sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 20 df             	andl   $0xffffffdf,(%eax)
83 60 04 ff          	andl   $0xffffffff,0x4(%eax)
c3                   	ret    

<bitass_s43_5_1(sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 08 20             	orl    $0x20,(%eax)
83 48 04 00          	orl    $0x0,0x4(%eax)
c3                   	ret    

<bitass_s43_5_t(sc_dt::sc_int<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 08 20             	orl    $0x20,(%eax)
83 48 04 00          	orl    $0x0,0x4(%eax)
c3                   	ret    

<bitass_s43_5_b(sc_dt::sc_int<43>&, bool)>:
83 ec 0c             	sub    $0xc,%esp
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 7c 24 10          	mov    0x10(%esp),%edi
0f b6 4c 24 14       	movzbl 0x14(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
31 db                	xor    %ebx,%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 07                	mov    (%edi),%eax
8b 57 04             	mov    0x4(%edi),%edx
0f a4 cb 05          	shld   $0x5,%ecx,%ebx
c1 e1 05             	shl    $0x5,%ecx
89 ce                	mov    %ecx,%esi
83 e0 df             	and    $0xffffffdf,%eax
09 c6                	or     %eax,%esi
89 d8                	mov    %ebx,%eax
09 d0                	or     %edx,%eax
89 37                	mov    %esi,(%edi)
89 47 04             	mov    %eax,0x4(%edi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
8b 7c 24 08          	mov    0x8(%esp),%edi
83 c4 0c             	add    $0xc,%esp
c3                   	ret    

<bitass_s43_42_0(sc_dt::sc_int<43>&)>:
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 51 04             	mov    0x4(%ecx),%edx
8b 01                	mov    (%ecx),%eax
80 e6 fb             	and    $0xfb,%dh
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 01                	mov    %eax,(%ecx)
89 51 04             	mov    %edx,0x4(%ecx)
c3                   	ret    

<bitass_s43_42_f(sc_dt::sc_int<43>&)>:
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 51 04             	mov    0x4(%ecx),%edx
8b 01                	mov    (%ecx),%eax
80 e6 fb             	and    $0xfb,%dh
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 01                	mov    %eax,(%ecx)
89 51 04             	mov    %edx,0x4(%ecx)
c3                   	ret    

<bitass_s43_42_1(sc_dt::sc_int<43>&)>:
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 51 04             	mov    0x4(%ecx),%edx
8b 01                	mov    (%ecx),%eax
80 ce 04             	or     $0x4,%dh
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 01                	mov    %eax,(%ecx)
89 51 04             	mov    %edx,0x4(%ecx)
c3                   	ret    

<bitass_s43_42_t(sc_dt::sc_int<43>&)>:
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 51 04             	mov    0x4(%ecx),%edx
8b 01                	mov    (%ecx),%eax
80 ce 04             	or     $0x4,%dh
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 01                	mov    %eax,(%ecx)
89 51 04             	mov    %edx,0x4(%ecx)
c3                   	ret    

<bitass_s43_42_b(sc_dt::sc_int<43>&, bool)>:
83 ec 08             	sub    $0x8,%esp
31 c9                	xor    %ecx,%ecx
8b 44 24 0c          	mov    0xc(%esp),%eax
0f b6 54 24 10       	movzbl 0x10(%esp),%edx
89 1c 24             	mov    %ebx,(%esp)
89 74 24 04          	mov    %esi,0x4(%esp)
8b 70 04             	mov    0x4(%eax),%esi
8b 18                	mov    (%eax),%ebx
89 d1                	mov    %edx,%ecx
ba 00 00 00 00       	mov    $0x0,%edx
c1 e1 0a             	shl    $0xa,%ecx
81 e6 ff fb ff ff    	and    $0xfffffbff,%esi
09 da                	or     %ebx,%edx
09 f1                	or     %esi,%ecx
0f a4 d1 15          	shld   $0x15,%edx,%ecx
c1 e2 15             	shl    $0x15,%edx
0f ac ca 15          	shrd   $0x15,%ecx,%edx
c1 f9 15             	sar    $0x15,%ecx
89 10                	mov    %edx,(%eax)
89 48 04             	mov    %ecx,0x4(%eax)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c3                   	ret    

<bitass_s43_si_0(sc_dt::sc_int<43>&, int)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
31 c0                	xor    %eax,%eax
8b 5c 24 08          	mov    0x8(%esp),%ebx
31 d2                	xor    %edx,%edx
f6 c1 20             	test   $0x20,%cl
0f 94 c0             	sete   %al
0f 95 c2             	setne  %dl
d3 e2                	shl    %cl,%edx
d3 e0                	shl    %cl,%eax
f7 d2                	not    %edx
f7 d0                	not    %eax
23 53 04             	and    0x4(%ebx),%edx
23 03                	and    (%ebx),%eax
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 03                	mov    %eax,(%ebx)
89 53 04             	mov    %edx,0x4(%ebx)
5b                   	pop    %ebx
c3                   	ret    

<bitass_s43_si_f(sc_dt::sc_int<43>&, int)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
31 c0                	xor    %eax,%eax
8b 5c 24 08          	mov    0x8(%esp),%ebx
31 d2                	xor    %edx,%edx
f6 c1 20             	test   $0x20,%cl
0f 94 c0             	sete   %al
0f 95 c2             	setne  %dl
d3 e2                	shl    %cl,%edx
d3 e0                	shl    %cl,%eax
f7 d2                	not    %edx
f7 d0                	not    %eax
23 53 04             	and    0x4(%ebx),%edx
23 03                	and    (%ebx),%eax
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 03                	mov    %eax,(%ebx)
89 53 04             	mov    %edx,0x4(%ebx)
5b                   	pop    %ebx
c3                   	ret    

<bitass_s43_si_1(sc_dt::sc_int<43>&, int)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
31 c0                	xor    %eax,%eax
8b 5c 24 08          	mov    0x8(%esp),%ebx
31 d2                	xor    %edx,%edx
f6 c1 20             	test   $0x20,%cl
0f 94 c0             	sete   %al
0f 95 c2             	setne  %dl
d3 e2                	shl    %cl,%edx
d3 e0                	shl    %cl,%eax
0b 53 04             	or     0x4(%ebx),%edx
0b 03                	or     (%ebx),%eax
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 03                	mov    %eax,(%ebx)
89 53 04             	mov    %edx,0x4(%ebx)
5b                   	pop    %ebx
c3                   	ret    

<bitass_s43_si_t(sc_dt::sc_int<43>&, int)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
31 c0                	xor    %eax,%eax
8b 5c 24 08          	mov    0x8(%esp),%ebx
31 d2                	xor    %edx,%edx
f6 c1 20             	test   $0x20,%cl
0f 94 c0             	sete   %al
0f 95 c2             	setne  %dl
d3 e2                	shl    %cl,%edx
d3 e0                	shl    %cl,%eax
0b 53 04             	or     0x4(%ebx),%edx
0b 03                	or     (%ebx),%eax
0f a4 c2 15          	shld   $0x15,%eax,%edx
c1 e0 15             	shl    $0x15,%eax
0f ac d0 15          	shrd   $0x15,%edx,%eax
c1 fa 15             	sar    $0x15,%edx
89 03                	mov    %eax,(%ebx)
89 53 04             	mov    %edx,0x4(%ebx)
5b                   	pop    %ebx
c3                   	ret    

<bitass_s43_si_b(sc_dt::sc_int<43>&, int, bool)>:
83 ec 10             	sub    $0x10,%esp
0f b6 4c 24 1c       	movzbl 0x1c(%esp),%ecx
89 7c 24 08          	mov    %edi,0x8(%esp)
89 6c 24 0c          	mov    %ebp,0xc(%esp)
31 ed                	xor    %ebp,%ebp
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 14          	mov    0x14(%esp),%esi
89 cf                	mov    %ecx,%edi
0f b6 4c 24 18       	movzbl 0x18(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
0f a5 fd             	shld   %cl,%edi,%ebp
d3 e7                	shl    %cl,%edi
f6 c1 20             	test   $0x20,%cl
74 04                	je     4204 <bitass_s43_si_b(sc_dt::sc_int<43>&, int, bool)+0x32>
89 fd                	mov    %edi,%ebp
31 ff                	xor    %edi,%edi
31 c0                	xor    %eax,%eax
31 d2                	xor    %edx,%edx
f6 c1 20             	test   $0x20,%cl
89 eb                	mov    %ebp,%ebx
0f 94 c0             	sete   %al
0f 95 c2             	setne  %dl
d3 e2                	shl    %cl,%edx
d3 e0                	shl    %cl,%eax
f7 d2                	not    %edx
f7 d0                	not    %eax
89 f9                	mov    %edi,%ecx
23 06                	and    (%esi),%eax
23 56 04             	and    0x4(%esi),%edx
09 c1                	or     %eax,%ecx
09 d3                	or     %edx,%ebx
0f a4 cb 15          	shld   $0x15,%ecx,%ebx
c1 e1 15             	shl    $0x15,%ecx
0f ac d9 15          	shrd   $0x15,%ebx,%ecx
c1 fb 15             	sar    $0x15,%ebx
89 0e                	mov    %ecx,(%esi)
89 5e 04             	mov    %ebx,0x4(%esi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
8b 7c 24 08          	mov    0x8(%esp),%edi
8b 6c 24 0c          	mov    0xc(%esp),%ebp
83 c4 10             	add    $0x10,%esp
c3                   	ret    

<bitass_u43_5_0(sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 20 df             	andl   $0xffffffdf,(%eax)
83 60 04 ff          	andl   $0xffffffff,0x4(%eax)
c3                   	ret    

<bitass_u43_5_f(sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 20 df             	andl   $0xffffffdf,(%eax)
83 60 04 ff          	andl   $0xffffffff,0x4(%eax)
c3                   	ret    

<bitass_u43_5_1(sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 08 20             	orl    $0x20,(%eax)
83 48 04 00          	orl    $0x0,0x4(%eax)
c3                   	ret    

<bitass_u43_5_t(sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 08 20             	orl    $0x20,(%eax)
83 48 04 00          	orl    $0x0,0x4(%eax)
c3                   	ret    

<bitass_u43_5_b(sc_dt::sc_uint<43>&, bool)>:
83 ec 0c             	sub    $0xc,%esp
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 7c 24 10          	mov    0x10(%esp),%edi
0f b6 4c 24 14       	movzbl 0x14(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
31 db                	xor    %ebx,%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 07                	mov    (%edi),%eax
8b 57 04             	mov    0x4(%edi),%edx
0f a4 cb 05          	shld   $0x5,%ecx,%ebx
c1 e1 05             	shl    $0x5,%ecx
89 ce                	mov    %ecx,%esi
83 e0 df             	and    $0xffffffdf,%eax
09 c6                	or     %eax,%esi
89 d8                	mov    %ebx,%eax
09 d0                	or     %edx,%eax
89 37                	mov    %esi,(%edi)
89 47 04             	mov    %eax,0x4(%edi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
8b 7c 24 08          	mov    0x8(%esp),%edi
83 c4 0c             	add    $0xc,%esp
c3                   	ret    

<bitass_u43_42_0(sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 20 ff             	andl   $0xffffffff,(%eax)
81 60 04 ff fb ff ff 	andl   $0xfffffbff,0x4(%eax)
c3                   	ret    
90                   	nop    

<bitass_u43_42_f(sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 20 ff             	andl   $0xffffffff,(%eax)
81 60 04 ff fb ff ff 	andl   $0xfffffbff,0x4(%eax)
c3                   	ret    
90                   	nop    

<bitass_u43_42_1(sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 08 00             	orl    $0x0,(%eax)
81 48 04 00 04 00 00 	orl    $0x400,0x4(%eax)
c3                   	ret    
90                   	nop    

<bitass_u43_42_t(sc_dt::sc_uint<43>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
83 08 00             	orl    $0x0,(%eax)
81 48 04 00 04 00 00 	orl    $0x400,0x4(%eax)
c3                   	ret    
90                   	nop    

<bitass_u43_42_b(sc_dt::sc_uint<43>&, bool)>:
83 ec 08             	sub    $0x8,%esp
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 0c          	mov    0xc(%esp),%esi
0f b6 4c 24 10       	movzbl 0x10(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
31 db                	xor    %ebx,%ebx
8b 56 04             	mov    0x4(%esi),%edx
89 cb                	mov    %ecx,%ebx
b9 00 00 00 00       	mov    $0x0,%ecx
c1 e3 0a             	shl    $0xa,%ebx
89 d8                	mov    %ebx,%eax
80 e6 fb             	and    $0xfb,%dh
09 d0                	or     %edx,%eax
89 46 04             	mov    %eax,0x4(%esi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c3                   	ret    
90                   	nop    

<bitass_u43_si_0(sc_dt::sc_uint<43>&, int)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
31 c0                	xor    %eax,%eax
8b 5c 24 08          	mov    0x8(%esp),%ebx
31 d2                	xor    %edx,%edx
f6 c1 20             	test   $0x20,%cl
0f 94 c0             	sete   %al
0f 95 c2             	setne  %dl
d3 e2                	shl    %cl,%edx
d3 e0                	shl    %cl,%eax
f7 d2                	not    %edx
f7 d0                	not    %eax
21 53 04             	and    %edx,0x4(%ebx)
21 03                	and    %eax,(%ebx)
5b                   	pop    %ebx
c3                   	ret    
90                   	nop    

<bitass_u43_si_f(sc_dt::sc_uint<43>&, int)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
31 c0                	xor    %eax,%eax
8b 5c 24 08          	mov    0x8(%esp),%ebx
31 d2                	xor    %edx,%edx
f6 c1 20             	test   $0x20,%cl
0f 94 c0             	sete   %al
0f 95 c2             	setne  %dl
d3 e2                	shl    %cl,%edx
d3 e0                	shl    %cl,%eax
f7 d2                	not    %edx
f7 d0                	not    %eax
21 53 04             	and    %edx,0x4(%ebx)
21 03                	and    %eax,(%ebx)
5b                   	pop    %ebx
c3                   	ret    
90                   	nop    

<bitass_u43_si_1(sc_dt::sc_uint<43>&, int)>:
83 ec 08             	sub    $0x8,%esp
8b 4c 24 10          	mov    0x10(%esp),%ecx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 44 24 0c          	mov    0xc(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
89 ce                	mov    %ecx,%esi
c1 ee 05             	shr    $0x5,%esi
83 e6 01             	and    $0x1,%esi
89 f3                	mov    %esi,%ebx
83 f3 01             	xor    $0x1,%ebx
d3 e3                	shl    %cl,%ebx
d3 e6                	shl    %cl,%esi
09 18                	or     %ebx,(%eax)
09 70 04             	or     %esi,0x4(%eax)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c3                   	ret    
90                   	nop    

<bitass_u43_si_t(sc_dt::sc_uint<43>&, int)>:
83 ec 08             	sub    $0x8,%esp
8b 4c 24 10          	mov    0x10(%esp),%ecx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 44 24 0c          	mov    0xc(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
89 ce                	mov    %ecx,%esi
c1 ee 05             	shr    $0x5,%esi
83 e6 01             	and    $0x1,%esi
89 f3                	mov    %esi,%ebx
83 f3 01             	xor    $0x1,%ebx
d3 e3                	shl    %cl,%ebx
d3 e6                	shl    %cl,%esi
09 18                	or     %ebx,(%eax)
09 70 04             	or     %esi,0x4(%eax)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c3                   	ret    
90                   	nop    

<bitass_u43_si_b(sc_dt::sc_uint<43>&, int, bool)>:
83 ec 14             	sub    $0x14,%esp
0f b6 4c 24 20       	movzbl 0x20(%esp),%ecx
c7 44 24 04 00 00 00 	movl   $0x0,0x4(%esp)
00 
8b 54 24 04          	mov    0x4(%esp),%edx
89 7c 24 10          	mov    %edi,0x10(%esp)
8b 7c 24 18          	mov    0x18(%esp),%edi
89 5c 24 08          	mov    %ebx,0x8(%esp)
89 0c 24             	mov    %ecx,(%esp)
0f b6 4c 24 1c       	movzbl 0x1c(%esp),%ecx
8b 04 24             	mov    (%esp),%eax
89 74 24 0c          	mov    %esi,0xc(%esp)
0f a5 c2             	shld   %cl,%eax,%edx
d3 e0                	shl    %cl,%eax
f6 c1 20             	test   $0x20,%cl
74 04                	je     4429 <bitass_u43_si_b(sc_dt::sc_uint<43>&, int, bool)+0x3d>
89 c2                	mov    %eax,%edx
31 c0                	xor    %eax,%eax
89 04 24             	mov    %eax,(%esp)
31 c0                	xor    %eax,%eax
8b 34 24             	mov    (%esp),%esi
89 54 24 04          	mov    %edx,0x4(%esp)
31 d2                	xor    %edx,%edx
f6 c1 20             	test   $0x20,%cl
0f 94 c0             	sete   %al
0f 95 c2             	setne  %dl
d3 e0                	shl    %cl,%eax
f7 d0                	not    %eax
23 07                	and    (%edi),%eax
d3 e2                	shl    %cl,%edx
f7 d2                	not    %edx
09 c6                	or     %eax,%esi
8b 44 24 04          	mov    0x4(%esp),%eax
23 57 04             	and    0x4(%edi),%edx
89 37                	mov    %esi,(%edi)
09 d0                	or     %edx,%eax
89 47 04             	mov    %eax,0x4(%edi)
8b 5c 24 08          	mov    0x8(%esp),%ebx
8b 74 24 0c          	mov    0xc(%esp),%esi
8b 7c 24 10          	mov    0x10(%esp),%edi
83 c4 14             	add    $0x14,%esp
c3                   	ret    

<bitass_s13_5_s11c_7(sc_dt::sc_int<13>&, sc_dt::sc_int<11> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 00                	mov    (%eax),%eax
8b 11                	mov    (%ecx),%edx
c1 e8 02             	shr    $0x2,%eax
83 e0 20             	and    $0x20,%eax
83 e2 df             	and    $0xffffffdf,%edx
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    

<bitass_s13_5_s11_7(sc_dt::sc_int<13>&, sc_dt::sc_int<11>&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 00                	mov    (%eax),%eax
8b 11                	mov    (%ecx),%edx
c1 e8 02             	shr    $0x2,%eax
83 e0 20             	and    $0x20,%eax
83 e2 df             	and    $0xffffffdf,%edx
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    

<bitass_s13_5_s13c_5(sc_dt::sc_int<13>&, sc_dt::sc_int<13> const&)>:
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 11                	mov    (%ecx),%edx
8b 00                	mov    (%eax),%eax
83 e2 df             	and    $0xffffffdf,%edx
83 e0 20             	and    $0x20,%eax
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<bitass_s13_5_s13_5(sc_dt::sc_int<13>&, sc_dt::sc_int<13>&)>:
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 11                	mov    (%ecx),%edx
8b 00                	mov    (%eax),%eax
83 e2 df             	and    $0xffffffdf,%edx
83 e0 20             	and    $0x20,%eax
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<bitassand_s13_5_b(sc_dt::sc_int<13>&, bool)>:
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 11                	mov    (%ecx),%edx
89 d0                	mov    %edx,%eax
83 e2 df             	and    $0xffffffdf,%edx
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
22 44 24 08          	and    0x8(%esp),%al
0f b6 c0             	movzbl %al,%eax
c1 e0 05             	shl    $0x5,%eax
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    

<bitassand_s13_5_s11c_7(sc_dt::sc_int<13>&, sc_dt::sc_int<11> const&)>:
53                   	push   %ebx
8b 5c 24 08          	mov    0x8(%esp),%ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 0b                	mov    (%ebx),%ecx
8b 10                	mov    (%eax),%edx
89 c8                	mov    %ecx,%eax
83 e1 df             	and    $0xffffffdf,%ecx
c1 f8 05             	sar    $0x5,%eax
c1 fa 07             	sar    $0x7,%edx
83 e0 01             	and    $0x1,%eax
21 c2                	and    %eax,%edx
c1 e2 05             	shl    $0x5,%edx
09 ca                	or     %ecx,%edx
89 13                	mov    %edx,(%ebx)
5b                   	pop    %ebx
c3                   	ret    

<bitassand_s13_5_s11_7(sc_dt::sc_int<13>&, sc_dt::sc_int<11>&)>:
53                   	push   %ebx
8b 5c 24 08          	mov    0x8(%esp),%ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 0b                	mov    (%ebx),%ecx
8b 10                	mov    (%eax),%edx
89 c8                	mov    %ecx,%eax
83 e1 df             	and    $0xffffffdf,%ecx
c1 f8 05             	sar    $0x5,%eax
c1 fa 07             	sar    $0x7,%edx
83 e0 01             	and    $0x1,%eax
21 c2                	and    %eax,%edx
c1 e2 05             	shl    $0x5,%edx
09 ca                	or     %ecx,%edx
89 13                	mov    %edx,(%ebx)
5b                   	pop    %ebx
c3                   	ret    

<bitassor_s13_5_b(sc_dt::sc_int<13>&, bool)>:
53                   	push   %ebx
8b 5c 24 08          	mov    0x8(%esp),%ebx
0f b6 54 24 0c       	movzbl 0xc(%esp),%edx
8b 0b                	mov    (%ebx),%ecx
89 c8                	mov    %ecx,%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
09 d0                	or     %edx,%eax
0f 95 c0             	setne  %al
83 e1 df             	and    $0xffffffdf,%ecx
0f b6 c0             	movzbl %al,%eax
c1 e0 05             	shl    $0x5,%eax
09 c8                	or     %ecx,%eax
89 03                	mov    %eax,(%ebx)
5b                   	pop    %ebx
c3                   	ret    

<bitassor_s13_5_s11c_7(sc_dt::sc_int<13>&, sc_dt::sc_int<11> const&)>:
53                   	push   %ebx
8b 5c 24 08          	mov    0x8(%esp),%ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 0b                	mov    (%ebx),%ecx
8b 10                	mov    (%eax),%edx
89 c8                	mov    %ecx,%eax
83 e1 df             	and    $0xffffffdf,%ecx
c1 fa 07             	sar    $0x7,%edx
c1 f8 05             	sar    $0x5,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c1 e0 05             	shl    $0x5,%eax
09 c8                	or     %ecx,%eax
89 03                	mov    %eax,(%ebx)
5b                   	pop    %ebx
c3                   	ret    

<bitassor_s13_5_s11_7(sc_dt::sc_int<13>&, sc_dt::sc_int<11>&)>:
53                   	push   %ebx
8b 5c 24 08          	mov    0x8(%esp),%ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 0b                	mov    (%ebx),%ecx
8b 10                	mov    (%eax),%edx
89 c8                	mov    %ecx,%eax
83 e1 df             	and    $0xffffffdf,%ecx
c1 fa 07             	sar    $0x7,%edx
c1 f8 05             	sar    $0x5,%eax
09 d0                	or     %edx,%eax
83 e0 01             	and    $0x1,%eax
c1 e0 05             	shl    $0x5,%eax
09 c8                	or     %ecx,%eax
89 03                	mov    %eax,(%ebx)
5b                   	pop    %ebx
c3                   	ret    

<bitassxor_s13_5_b(sc_dt::sc_int<13>&, bool)>:
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 11                	mov    (%ecx),%edx
89 d0                	mov    %edx,%eax
c1 f8 05             	sar    $0x5,%eax
83 e0 01             	and    $0x1,%eax
38 44 24 08          	cmp    %al,0x8(%esp)
0f 95 c0             	setne  %al
83 e2 df             	and    $0xffffffdf,%edx
0f b6 c0             	movzbl %al,%eax
c1 e0 05             	shl    $0x5,%eax
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<bitassxor_s13_5_s11c_7(sc_dt::sc_int<13>&, sc_dt::sc_int<11> const&)>:
53                   	push   %ebx
8b 5c 24 08          	mov    0x8(%esp),%ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 0b                	mov    (%ebx),%ecx
8b 10                	mov    (%eax),%edx
89 c8                	mov    %ecx,%eax
c1 fa 07             	sar    $0x7,%edx
c1 f8 05             	sar    $0x5,%eax
83 e2 01             	and    $0x1,%edx
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
83 e1 df             	and    $0xffffffdf,%ecx
0f b6 c0             	movzbl %al,%eax
c1 e0 05             	shl    $0x5,%eax
09 c8                	or     %ecx,%eax
89 03                	mov    %eax,(%ebx)
5b                   	pop    %ebx
c3                   	ret    
90                   	nop    

<bitassxor_s13_5_s11_7(sc_dt::sc_int<13>&, sc_dt::sc_int<11>&)>:
53                   	push   %ebx
8b 5c 24 08          	mov    0x8(%esp),%ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 0b                	mov    (%ebx),%ecx
8b 00                	mov    (%eax),%eax
89 ca                	mov    %ecx,%edx
c1 fa 05             	sar    $0x5,%edx
c1 f8 07             	sar    $0x7,%eax
83 e2 01             	and    $0x1,%edx
83 e0 01             	and    $0x1,%eax
38 c2                	cmp    %al,%dl
0f 95 c0             	setne  %al
83 e1 df             	and    $0xffffffdf,%ecx
0f b6 c0             	movzbl %al,%eax
c1 e0 05             	shl    $0x5,%eax
09 c8                	or     %ecx,%eax
89 03                	mov    %eax,(%ebx)
5b                   	pop    %ebx
c3                   	ret    
90                   	nop    

<subref_s13c_7_3_si(sc_dt::sc_int<13> const&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
c3                   	ret    
90                   	nop    

<subref_s13_7_3_si(sc_dt::sc_int<13>&)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
c3                   	ret    
90                   	nop    

<subref_s43c_7_3_si(sc_dt::sc_int<43> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 58 04             	mov    0x4(%eax),%ebx
8b 08                	mov    (%eax),%ecx
0f ac d9 03          	shrd   $0x3,%ebx,%ecx
c1 fb 03             	sar    $0x3,%ebx
89 c8                	mov    %ecx,%eax
5b                   	pop    %ebx
83 e0 1f             	and    $0x1f,%eax
c3                   	ret    

<subref_s43_7_3_si(sc_dt::sc_int<43>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 58 04             	mov    0x4(%eax),%ebx
8b 08                	mov    (%eax),%ecx
0f ac d9 03          	shrd   $0x3,%ebx,%ecx
c1 fb 03             	sar    $0x3,%ebx
89 c8                	mov    %ecx,%eax
5b                   	pop    %ebx
83 e0 1f             	and    $0x1f,%eax
c3                   	ret    

<subass_u4_s13c_7_3(sc_dt::sc_uint<4>&, sc_dt::sc_int<13> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 54 24 04          	mov    0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 0f             	and    $0xf,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<subass_u4_s13_7_3(sc_dt::sc_uint<4>&, sc_dt::sc_int<13>&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 54 24 04          	mov    0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 0f             	and    $0xf,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<subass_u5_s13c_7_3(sc_dt::sc_uint<5>&, sc_dt::sc_int<13> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 54 24 04          	mov    0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<subass_u5_s13_7_3(sc_dt::sc_uint<5>&, sc_dt::sc_int<13>&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 54 24 04          	mov    0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<subass_u6_s13c_7_3(sc_dt::sc_uint<6>&, sc_dt::sc_int<13> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 54 24 04          	mov    0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<subass_u6_s13_7_3(sc_dt::sc_uint<6>&, sc_dt::sc_int<13>&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 54 24 04          	mov    0x4(%esp),%edx
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    
90                   	nop    

<subass_s13_7_3_10(sc_dt::sc_int<13>&)>:
8b 54 24 04          	mov    0x4(%esp),%edx
8b 02                	mov    (%edx),%eax
24 07                	and    $0x7,%al
83 c8 50             	or     $0x50,%eax
89 02                	mov    %eax,(%edx)
c3                   	ret    

<subass_s13_7_3_si(sc_dt::sc_int<13>&, int)>:
8b 44 24 04          	mov    0x4(%esp),%eax
8b 54 24 08          	mov    0x8(%esp),%edx
8b 08                	mov    (%eax),%ecx
83 e2 1f             	and    $0x1f,%edx
c1 e2 03             	shl    $0x3,%edx
80 e1 07             	and    $0x7,%cl
09 ca                	or     %ecx,%edx
89 10                	mov    %edx,(%eax)
c3                   	ret    

<subass_s13_7_3_s11c_1(sc_dt::sc_int<13>&, sc_dt::sc_int<11> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 00                	mov    (%eax),%eax
8b 11                	mov    (%ecx),%edx
c1 e0 02             	shl    $0x2,%eax
83 e0 08             	and    $0x8,%eax
80 e2 07             	and    $0x7,%dl
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    

<subass_s13_7_3_s11_1(sc_dt::sc_int<13>&, sc_dt::sc_int<11>&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 00                	mov    (%eax),%eax
8b 11                	mov    (%ecx),%edx
c1 e0 02             	shl    $0x2,%eax
83 e0 08             	and    $0x8,%eax
80 e2 07             	and    $0x7,%dl
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    

<subass_s13_7_3_s11c_8_4(sc_dt::sc_int<13>&, sc_dt::sc_int<11> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 00                	mov    (%eax),%eax
8b 11                	mov    (%ecx),%edx
d1 e8                	shr    %eax
25 f8 00 00 00       	and    $0xf8,%eax
80 e2 07             	and    $0x7,%dl
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<subass_s13_7_3_s11_8_4(sc_dt::sc_int<13>&, sc_dt::sc_int<11>&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 00                	mov    (%eax),%eax
8b 11                	mov    (%ecx),%edx
d1 e8                	shr    %eax
25 f8 00 00 00       	and    $0xf8,%eax
80 e2 07             	and    $0x7,%dl
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    
90                   	nop    

<subass_s11_8_4_s11c_8_4(sc_dt::sc_int<11>&, sc_dt::sc_int<11> const&)>:
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 11                	mov    (%ecx),%edx
8b 00                	mov    (%eax),%eax
81 e2 0f fe ff ff    	and    $0xfffffe0f,%edx
25 f0 01 00 00       	and    $0x1f0,%eax
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    

<subass_s11_8_4_s11_8_4(sc_dt::sc_int<11>&, sc_dt::sc_int<11>&)>:
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 11                	mov    (%ecx),%edx
8b 00                	mov    (%eax),%eax
81 e2 0f fe ff ff    	and    $0xfffffe0f,%edx
25 f0 01 00 00       	and    $0x1f0,%eax
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    

<subass_s13_7_3_u4c(sc_dt::sc_int<13>&, sc_dt::sc_uint<4> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 00                	mov    (%eax),%eax
8b 11                	mov    (%ecx),%edx
83 e0 1f             	and    $0x1f,%eax
c1 e0 03             	shl    $0x3,%eax
80 e2 07             	and    $0x7,%dl
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    

<subass_s13_7_3_u5c(sc_dt::sc_int<13>&, sc_dt::sc_uint<5> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 00                	mov    (%eax),%eax
8b 11                	mov    (%ecx),%edx
83 e0 1f             	and    $0x1f,%eax
c1 e0 03             	shl    $0x3,%eax
80 e2 07             	and    $0x7,%dl
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    

<subass_s13_7_3_u6c(sc_dt::sc_int<13>&, sc_dt::sc_uint<6> const&)>:
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 04          	mov    0x4(%esp),%ecx
8b 00                	mov    (%eax),%eax
8b 11                	mov    (%ecx),%edx
83 e0 1f             	and    $0x1f,%eax
c1 e0 03             	shl    $0x3,%eax
80 e2 07             	and    $0x7,%dl
09 d0                	or     %edx,%eax
89 01                	mov    %eax,(%ecx)
c3                   	ret    

<concat_bool_bitr(bool, sc_dt::sc_int<15> const&)>:
83 ec 08             	sub    $0x8,%esp
31 d2                	xor    %edx,%edx
8b 4c 24 10          	mov    0x10(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
0f b6 44 24 0c       	movzbl 0xc(%esp),%eax
89 74 24 04          	mov    %esi,0x4(%esp)
8b 1c 24             	mov    (%esp),%ebx
8b 31                	mov    (%ecx),%esi
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
d1 fe                	sar    %esi
89 f1                	mov    %esi,%ecx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
83 e1 01             	and    $0x1,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_bool_subr(bool, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
31 d2                	xor    %edx,%edx
0f b6 44 24 08       	movzbl 0x8(%esp),%eax
5b                   	pop    %ebx
8b 09                	mov    (%ecx),%ecx
0f a4 c2 04          	shld   $0x4,%eax,%edx
c1 e0 04             	shl    $0x4,%eax
c1 f9 05             	sar    $0x5,%ecx
83 e1 0f             	and    $0xf,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_bool_intc(bool, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
31 d2                	xor    %edx,%edx
0f b6 44 24 08       	movzbl 0x8(%esp),%eax
5b                   	pop    %ebx
8b 09                	mov    (%ecx),%ecx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
c1 e0 0f             	shl    $0xf,%eax
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_bitr_bool(sc_dt::sc_int<13> const&, bool)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
31 d2                	xor    %edx,%edx
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bitr_bitr(sc_dt::sc_int<13> const&, sc_dt::sc_int<15> const&)>:
83 ec 08             	sub    $0x8,%esp
31 d2                	xor    %edx,%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
8b 1c 24             	mov    (%esp),%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
83 e0 01             	and    $0x1,%eax
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
8b 31                	mov    (%ecx),%esi
d1 fe                	sar    %esi
89 f1                	mov    %esi,%ecx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
83 e1 01             	and    $0x1,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bitr_subr(sc_dt::sc_int<13> const&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
31 d2                	xor    %edx,%edx
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f a4 c2 04          	shld   $0x4,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 04             	shl    $0x4,%eax
c1 f9 05             	sar    $0x5,%ecx
83 e1 0f             	and    $0xf,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bitr_intc(sc_dt::sc_int<13> const&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
31 d2                	xor    %edx,%edx
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 0f             	shl    $0xf,%eax
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_subr_bool(sc_dt::sc_int<13> const&, bool)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_subr_bitr(sc_dt::sc_int<13> const&, sc_dt::sc_int<15> const&)>:
83 ec 08             	sub    $0x8,%esp
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
8b 1c 24             	mov    (%esp),%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 00                	mov    (%eax),%eax
8b 31                	mov    (%ecx),%esi
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
d1 fe                	sar    %esi
89 c2                	mov    %eax,%edx
89 f1                	mov    %esi,%ecx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c1 fa 1f             	sar    $0x1f,%edx
83 e1 01             	and    $0x1,%ecx
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_subr_subr(sc_dt::sc_int<13> const&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
c1 f9 05             	sar    $0x5,%ecx
0f a4 c2 04          	shld   $0x4,%eax,%edx
83 e1 0f             	and    $0xf,%ecx
c1 e0 04             	shl    $0x4,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_subr_intc(sc_dt::sc_int<13> const&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
c1 e0 0f             	shl    $0xf,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_intc_bool(sc_dt::sc_int<13> const&, bool)>:
53                   	push   %ebx
8b 4c 24 08          	mov    0x8(%esp),%ecx
0f b6 44 24 0c       	movzbl 0xc(%esp),%eax
8b 09                	mov    (%ecx),%ecx
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 01          	shld   $0x1,%ecx,%ebx
01 c9                	add    %ecx,%ecx
5b                   	pop    %ebx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_intc_bitr(sc_dt::sc_int<13> const&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 08                	mov    (%eax),%ecx
d1 f9                	sar    %ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
83 e0 01             	and    $0x1,%eax
8b 09                	mov    (%ecx),%ecx
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 01          	shld   $0x1,%ecx,%ebx
01 c9                	add    %ecx,%ecx
5b                   	pop    %ebx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_intc_subr(sc_dt::sc_int<13> const&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 08                	mov    (%eax),%ecx
8b 44 24 0c          	mov    0xc(%esp),%eax
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
8b 00                	mov    (%eax),%eax
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 04          	shld   $0x4,%ecx,%ebx
c1 e1 04             	shl    $0x4,%ecx
c1 f8 05             	sar    $0x5,%eax
83 e0 0f             	and    $0xf,%eax
5b                   	pop    %ebx
09 c1                	or     %eax,%ecx
89 c8                	mov    %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_intc_intc(sc_dt::sc_int<13> const&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
25 ff 1f 00 00       	and    $0x1fff,%eax
89 c2                	mov    %eax,%edx
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
c1 e0 0f             	shl    $0xf,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_intc_intc(sc_dt::sc_int<13> const&, sc_dt::sc_int<13> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
25 ff 1f 00 00       	and    $0x1fff,%eax
89 c2                	mov    %eax,%edx
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 0d          	shld   $0xd,%eax,%edx
c1 e0 0d             	shl    $0xd,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bool_bit(bool, sc_dt::sc_int<15>&)>:
83 ec 08             	sub    $0x8,%esp
31 d2                	xor    %edx,%edx
8b 4c 24 10          	mov    0x10(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
0f b6 44 24 0c       	movzbl 0xc(%esp),%eax
89 74 24 04          	mov    %esi,0x4(%esp)
8b 1c 24             	mov    (%esp),%ebx
8b 31                	mov    (%ecx),%esi
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
d1 fe                	sar    %esi
89 f1                	mov    %esi,%ecx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
83 e1 01             	and    $0x1,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_bool_sub(bool, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
31 d2                	xor    %edx,%edx
0f b6 44 24 08       	movzbl 0x8(%esp),%eax
5b                   	pop    %ebx
8b 09                	mov    (%ecx),%ecx
0f a4 c2 04          	shld   $0x4,%eax,%edx
c1 e0 04             	shl    $0x4,%eax
c1 f9 05             	sar    $0x5,%ecx
83 e1 0f             	and    $0xf,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_bool_int(bool, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
31 d2                	xor    %edx,%edx
0f b6 44 24 08       	movzbl 0x8(%esp),%eax
5b                   	pop    %ebx
8b 09                	mov    (%ecx),%ecx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
c1 e0 0f             	shl    $0xf,%eax
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_bitr_bit(sc_dt::sc_int<13> const&, sc_dt::sc_int<15>&)>:
83 ec 08             	sub    $0x8,%esp
31 d2                	xor    %edx,%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
8b 1c 24             	mov    (%esp),%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
83 e0 01             	and    $0x1,%eax
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
8b 31                	mov    (%ecx),%esi
d1 fe                	sar    %esi
89 f1                	mov    %esi,%ecx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
83 e1 01             	and    $0x1,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bitr_sub(sc_dt::sc_int<13> const&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
31 d2                	xor    %edx,%edx
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f a4 c2 04          	shld   $0x4,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 04             	shl    $0x4,%eax
c1 f9 05             	sar    $0x5,%ecx
83 e1 0f             	and    $0xf,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bitr_int(sc_dt::sc_int<13> const&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
31 d2                	xor    %edx,%edx
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 0f             	shl    $0xf,%eax
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_subr_bit(sc_dt::sc_int<13> const&, sc_dt::sc_int<15>&)>:
83 ec 08             	sub    $0x8,%esp
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
8b 1c 24             	mov    (%esp),%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 00                	mov    (%eax),%eax
8b 31                	mov    (%ecx),%esi
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
d1 fe                	sar    %esi
89 c2                	mov    %eax,%edx
89 f1                	mov    %esi,%ecx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c1 fa 1f             	sar    $0x1f,%edx
83 e1 01             	and    $0x1,%ecx
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_subr_sub(sc_dt::sc_int<13> const&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
c1 f9 05             	sar    $0x5,%ecx
0f a4 c2 04          	shld   $0x4,%eax,%edx
83 e1 0f             	and    $0xf,%ecx
c1 e0 04             	shl    $0x4,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_subr_int(sc_dt::sc_int<13> const&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
c1 e0 0f             	shl    $0xf,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_intc_bit(sc_dt::sc_int<13> const&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 08                	mov    (%eax),%ecx
d1 f9                	sar    %ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
83 e0 01             	and    $0x1,%eax
8b 09                	mov    (%ecx),%ecx
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 01          	shld   $0x1,%ecx,%ebx
01 c9                	add    %ecx,%ecx
5b                   	pop    %ebx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_intc_sub(sc_dt::sc_int<13> const&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 08                	mov    (%eax),%ecx
8b 44 24 0c          	mov    0xc(%esp),%eax
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
8b 00                	mov    (%eax),%eax
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 04          	shld   $0x4,%ecx,%ebx
c1 e1 04             	shl    $0x4,%ecx
c1 f8 05             	sar    $0x5,%eax
83 e0 0f             	and    $0xf,%eax
5b                   	pop    %ebx
09 c1                	or     %eax,%ecx
89 c8                	mov    %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_intc_int(sc_dt::sc_int<13> const&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
25 ff 1f 00 00       	and    $0x1fff,%eax
89 c2                	mov    %eax,%edx
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
c1 e0 0f             	shl    $0xf,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bit_bool(sc_dt::sc_int<13>&, bool)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
31 d2                	xor    %edx,%edx
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bit_bitr(sc_dt::sc_int<13>&, sc_dt::sc_int<15> const&)>:
83 ec 08             	sub    $0x8,%esp
31 d2                	xor    %edx,%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
8b 1c 24             	mov    (%esp),%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
83 e0 01             	and    $0x1,%eax
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
8b 31                	mov    (%ecx),%esi
d1 fe                	sar    %esi
89 f1                	mov    %esi,%ecx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
83 e1 01             	and    $0x1,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bit_subr(sc_dt::sc_int<13>&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
31 d2                	xor    %edx,%edx
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f a4 c2 04          	shld   $0x4,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 04             	shl    $0x4,%eax
c1 f9 05             	sar    $0x5,%ecx
83 e1 0f             	and    $0xf,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bit_intc(sc_dt::sc_int<13>&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
31 d2                	xor    %edx,%edx
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 0f             	shl    $0xf,%eax
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_sub_bool(sc_dt::sc_int<13>&, bool)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
0f b6 4c 24 0c       	movzbl 0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_sub_bitr(sc_dt::sc_int<13>&, sc_dt::sc_int<15> const&)>:
83 ec 08             	sub    $0x8,%esp
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
8b 1c 24             	mov    (%esp),%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 00                	mov    (%eax),%eax
8b 31                	mov    (%ecx),%esi
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
d1 fe                	sar    %esi
89 c2                	mov    %eax,%edx
89 f1                	mov    %esi,%ecx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c1 fa 1f             	sar    $0x1f,%edx
83 e1 01             	and    $0x1,%ecx
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_sub_subr(sc_dt::sc_int<13>&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
c1 f9 05             	sar    $0x5,%ecx
0f a4 c2 04          	shld   $0x4,%eax,%edx
83 e1 0f             	and    $0xf,%ecx
c1 e0 04             	shl    $0x4,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_sub_intc(sc_dt::sc_int<13>&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
c1 e0 0f             	shl    $0xf,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_int_bool(sc_dt::sc_int<13>&, bool)>:
53                   	push   %ebx
8b 4c 24 08          	mov    0x8(%esp),%ecx
0f b6 44 24 0c       	movzbl 0xc(%esp),%eax
8b 09                	mov    (%ecx),%ecx
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 01          	shld   $0x1,%ecx,%ebx
01 c9                	add    %ecx,%ecx
5b                   	pop    %ebx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_int_bitr(sc_dt::sc_int<13>&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 08                	mov    (%eax),%ecx
d1 f9                	sar    %ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
83 e0 01             	and    $0x1,%eax
8b 09                	mov    (%ecx),%ecx
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 01          	shld   $0x1,%ecx,%ebx
01 c9                	add    %ecx,%ecx
5b                   	pop    %ebx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_int_subr(sc_dt::sc_int<13>&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 08                	mov    (%eax),%ecx
8b 44 24 0c          	mov    0xc(%esp),%eax
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
8b 00                	mov    (%eax),%eax
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 04          	shld   $0x4,%ecx,%ebx
c1 e1 04             	shl    $0x4,%ecx
c1 f8 05             	sar    $0x5,%eax
83 e0 0f             	and    $0xf,%eax
5b                   	pop    %ebx
09 c1                	or     %eax,%ecx
89 c8                	mov    %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_int_intc(sc_dt::sc_int<13>&, sc_dt::sc_int<15> const&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
25 ff 1f 00 00       	and    $0x1fff,%eax
89 c2                	mov    %eax,%edx
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
c1 e0 0f             	shl    $0xf,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bool_cat(bool, sc_dt::sc_int<10>&, sc_dt::sc_int<11>&)>:
53                   	push   %ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
31 db                	xor    %ebx,%ebx
0f b6 4c 24 08       	movzbl 0x8(%esp),%ecx
8b 00                	mov    (%eax),%eax
0f a4 cb 0b          	shld   $0xb,%ecx,%ebx
c1 e1 0b             	shl    $0xb,%ecx
c1 f8 02             	sar    $0x2,%eax
25 ff 00 00 00       	and    $0xff,%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 03          	shld   $0x3,%eax,%edx
c1 e0 03             	shl    $0x3,%eax
09 c8                	or     %ecx,%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
5b                   	pop    %ebx
8b 09                	mov    (%ecx),%ecx
c1 f9 03             	sar    $0x3,%ecx
83 e1 07             	and    $0x7,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_bitr_cat(sc_dt::sc_int<13> const&, sc_dt::sc_int<10>&, sc_dt::sc_int<11>&)>:
56                   	push   %esi
31 d2                	xor    %edx,%edx
53                   	push   %ebx
8b 44 24 10          	mov    0x10(%esp),%eax
8b 08                	mov    (%eax),%ecx
8b 44 24 14          	mov    0x14(%esp),%eax
c1 f9 02             	sar    $0x2,%ecx
8b 00                	mov    (%eax),%eax
81 e1 ff 00 00 00    	and    $0xff,%ecx
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 03          	shld   $0x3,%ecx,%ebx
c1 f8 03             	sar    $0x3,%eax
83 e0 07             	and    $0x7,%eax
c1 e1 03             	shl    $0x3,%ecx
09 c1                	or     %eax,%ecx
8b 44 24 0c          	mov    0xc(%esp),%eax
5b                   	pop    %ebx
8b 30                	mov    (%eax),%esi
c1 fe 02             	sar    $0x2,%esi
89 f0                	mov    %esi,%eax
83 e0 01             	and    $0x1,%eax
0f a4 c2 0b          	shld   $0xb,%eax,%edx
5e                   	pop    %esi
c1 e0 0b             	shl    $0xb,%eax
09 c1                	or     %eax,%ecx
89 c8                	mov    %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_subr_cat(sc_dt::sc_int<13> const&, sc_dt::sc_int<10>&, sc_dt::sc_int<11>&)>:
53                   	push   %ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 08                	mov    (%eax),%ecx
8b 44 24 08          	mov    0x8(%esp),%eax
c1 f9 02             	sar    $0x2,%ecx
8b 00                	mov    (%eax),%eax
81 e1 ff 00 00 00    	and    $0xff,%ecx
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 03          	shld   $0x3,%ecx,%ebx
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
c1 e1 03             	shl    $0x3,%ecx
0f a4 c2 0b          	shld   $0xb,%eax,%edx
c1 e0 0b             	shl    $0xb,%eax
09 c1                	or     %eax,%ecx
8b 44 24 10          	mov    0x10(%esp),%eax
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 07             	and    $0x7,%eax
09 c1                	or     %eax,%ecx
89 c8                	mov    %ecx,%eax
c3                   	ret    

<concat_intc_cat(sc_dt::sc_int<13> const&, sc_dt::sc_int<10>&, sc_dt::sc_int<11>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 08                	mov    (%eax),%ecx
8b 44 24 10          	mov    0x10(%esp),%eax
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
8b 00                	mov    (%eax),%eax
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 0b          	shld   $0xb,%ecx,%ebx
c1 e1 0b             	shl    $0xb,%ecx
c1 f8 03             	sar    $0x3,%eax
83 e0 07             	and    $0x7,%eax
09 c1                	or     %eax,%ecx
8b 44 24 0c          	mov    0xc(%esp),%eax
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
c1 f8 02             	sar    $0x2,%eax
25 ff 00 00 00       	and    $0xff,%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 03          	shld   $0x3,%eax,%edx
c1 e0 03             	shl    $0x3,%eax
09 c1                	or     %eax,%ecx
89 c8                	mov    %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_cat_bool(sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, sc_dt::sc_int<15> const&)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 10          	mov    0x10(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
89 7c 24 08          	mov    %edi,0x8(%esp)
89 74 24 04          	mov    %esi,0x4(%esp)
8b 30                	mov    (%eax),%esi
8b 44 24 14          	mov    0x14(%esp),%eax
c1 fe 02             	sar    $0x2,%esi
8b 08                	mov    (%eax),%ecx
81 e6 ff 00 00 00    	and    $0xff,%esi
89 f7                	mov    %esi,%edi
c1 ff 1f             	sar    $0x1f,%edi
0f a4 f7 03          	shld   $0x3,%esi,%edi
c1 f9 03             	sar    $0x3,%ecx
89 fa                	mov    %edi,%edx
8b 7c 24 08          	mov    0x8(%esp),%edi
c1 e6 03             	shl    $0x3,%esi
83 e1 07             	and    $0x7,%ecx
89 f0                	mov    %esi,%eax
89 cb                	mov    %ecx,%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
09 c8                	or     %ecx,%eax
8b 4c 24 18          	mov    0x18(%esp),%ecx
c1 fb 1f             	sar    $0x1f,%ebx
09 da                	or     %ebx,%edx
8b 1c 24             	mov    (%esp),%ebx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 0f             	shl    $0xf,%eax
83 c4 0c             	add    $0xc,%esp
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_cat_bitr(sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, sc_dt::sc_int<15> const&)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 10          	mov    0x10(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
89 7c 24 08          	mov    %edi,0x8(%esp)
89 74 24 04          	mov    %esi,0x4(%esp)
8b 30                	mov    (%eax),%esi
8b 44 24 14          	mov    0x14(%esp),%eax
c1 fe 02             	sar    $0x2,%esi
8b 08                	mov    (%eax),%ecx
81 e6 ff 00 00 00    	and    $0xff,%esi
89 f7                	mov    %esi,%edi
c1 ff 1f             	sar    $0x1f,%edi
0f a4 f7 03          	shld   $0x3,%esi,%edi
c1 f9 03             	sar    $0x3,%ecx
89 fa                	mov    %edi,%edx
8b 7c 24 08          	mov    0x8(%esp),%edi
c1 e6 03             	shl    $0x3,%esi
83 e1 07             	and    $0x7,%ecx
89 f0                	mov    %esi,%eax
89 cb                	mov    %ecx,%ebx
09 c8                	or     %ecx,%eax
8b 4c 24 18          	mov    0x18(%esp),%ecx
c1 fb 1f             	sar    $0x1f,%ebx
09 da                	or     %ebx,%edx
8b 1c 24             	mov    (%esp),%ebx
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
8b 31                	mov    (%ecx),%esi
d1 fe                	sar    %esi
89 f1                	mov    %esi,%ecx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 0c             	add    $0xc,%esp
83 e1 01             	and    $0x1,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_cat_subr(sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, sc_dt::sc_int<15> const&)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 10          	mov    0x10(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
89 7c 24 08          	mov    %edi,0x8(%esp)
89 74 24 04          	mov    %esi,0x4(%esp)
8b 30                	mov    (%eax),%esi
8b 44 24 14          	mov    0x14(%esp),%eax
c1 fe 02             	sar    $0x2,%esi
8b 08                	mov    (%eax),%ecx
81 e6 ff 00 00 00    	and    $0xff,%esi
89 f7                	mov    %esi,%edi
c1 ff 1f             	sar    $0x1f,%edi
0f a4 f7 03          	shld   $0x3,%esi,%edi
c1 f9 03             	sar    $0x3,%ecx
89 fa                	mov    %edi,%edx
8b 7c 24 08          	mov    0x8(%esp),%edi
c1 e6 03             	shl    $0x3,%esi
83 e1 07             	and    $0x7,%ecx
89 f0                	mov    %esi,%eax
89 cb                	mov    %ecx,%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
09 c8                	or     %ecx,%eax
8b 4c 24 18          	mov    0x18(%esp),%ecx
c1 fb 1f             	sar    $0x1f,%ebx
09 da                	or     %ebx,%edx
8b 1c 24             	mov    (%esp),%ebx
0f a4 c2 04          	shld   $0x4,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 04             	shl    $0x4,%eax
83 c4 0c             	add    $0xc,%esp
c1 f9 05             	sar    $0x5,%ecx
83 e1 0f             	and    $0xf,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_cat_intc(sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, sc_dt::sc_int<15> const&)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 10          	mov    0x10(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
89 7c 24 08          	mov    %edi,0x8(%esp)
89 74 24 04          	mov    %esi,0x4(%esp)
8b 30                	mov    (%eax),%esi
8b 44 24 14          	mov    0x14(%esp),%eax
c1 fe 02             	sar    $0x2,%esi
8b 08                	mov    (%eax),%ecx
81 e6 ff 00 00 00    	and    $0xff,%esi
89 f7                	mov    %esi,%edi
c1 ff 1f             	sar    $0x1f,%edi
0f a4 f7 03          	shld   $0x3,%esi,%edi
c1 f9 03             	sar    $0x3,%ecx
89 fa                	mov    %edi,%edx
8b 7c 24 08          	mov    0x8(%esp),%edi
c1 e6 03             	shl    $0x3,%esi
83 e1 07             	and    $0x7,%ecx
89 f0                	mov    %esi,%eax
89 cb                	mov    %ecx,%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
09 c8                	or     %ecx,%eax
8b 4c 24 18          	mov    0x18(%esp),%ecx
c1 fb 1f             	sar    $0x1f,%ebx
09 da                	or     %ebx,%edx
8b 1c 24             	mov    (%esp),%ebx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 0f             	shl    $0xf,%eax
83 c4 0c             	add    $0xc,%esp
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concatl_bit_bit(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&, int)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 18          	mov    0x18(%esp),%eax
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 14          	mov    0x14(%esp),%esi
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 7c 24 10          	mov    0x10(%esp),%edi
89 1c 24             	mov    %ebx,(%esp)
89 c2                	mov    %eax,%edx
89 c1                	mov    %eax,%ecx
8b 1e                	mov    (%esi),%ebx
c1 fa 1f             	sar    $0x1f,%edx
09 d1                	or     %edx,%ecx
0f 95 c1             	setne  %cl
0f b6 c9             	movzbl %cl,%ecx
83 e3 fd             	and    $0xfffffffd,%ebx
01 c9                	add    %ecx,%ecx
09 d9                	or     %ebx,%ecx
0f ac d0 01          	shrd   $0x1,%edx,%eax
d1 ea                	shr    %edx
89 0e                	mov    %ecx,(%esi)
09 d0                	or     %edx,%eax
8b 17                	mov    (%edi),%edx
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c1 e0 02             	shl    $0x2,%eax
83 e2 fb             	and    $0xfffffffb,%edx
09 d0                	or     %edx,%eax
89 07                	mov    %eax,(%edi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
8b 7c 24 08          	mov    0x8(%esp),%edi
83 c4 0c             	add    $0xc,%esp
c3                   	ret    
90                   	nop    

<concatl_bit_sub(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&, int)>:
83 ec 0c             	sub    $0xc,%esp
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 14          	mov    0x14(%esp),%esi
8b 44 24 18          	mov    0x18(%esp),%eax
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 7c 24 10          	mov    0x10(%esp),%edi
89 1c 24             	mov    %ebx,(%esp)
8b 1e                	mov    (%esi),%ebx
89 c1                	mov    %eax,%ecx
89 c2                	mov    %eax,%edx
83 e1 0f             	and    $0xf,%ecx
c1 fa 1f             	sar    $0x1f,%edx
81 e3 1f fe ff ff    	and    $0xfffffe1f,%ebx
c1 e1 05             	shl    $0x5,%ecx
09 d9                	or     %ebx,%ecx
0f ac d0 04          	shrd   $0x4,%edx,%eax
c1 ea 04             	shr    $0x4,%edx
89 0e                	mov    %ecx,(%esi)
09 d0                	or     %edx,%eax
8b 17                	mov    (%edi),%edx
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c1 e0 02             	shl    $0x2,%eax
83 e2 fb             	and    $0xfffffffb,%edx
09 d0                	or     %edx,%eax
89 07                	mov    %eax,(%edi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
8b 7c 24 08          	mov    0x8(%esp),%edi
83 c4 0c             	add    $0xc,%esp
c3                   	ret    
90                   	nop    

<concatl_bit_int(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&, int)>:
83 ec 08             	sub    $0x8,%esp
8b 44 24 14          	mov    0x14(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
8b 5c 24 10          	mov    0x10(%esp),%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 0c          	mov    0xc(%esp),%esi
89 c1                	mov    %eax,%ecx
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
c1 e1 11             	shl    $0x11,%ecx
0f ac d0 0f          	shrd   $0xf,%edx,%eax
c1 f9 11             	sar    $0x11,%ecx
c1 ea 0f             	shr    $0xf,%edx
09 d0                	or     %edx,%eax
89 0b                	mov    %ecx,(%ebx)
8b 16                	mov    (%esi),%edx
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c1 e0 02             	shl    $0x2,%eax
83 e2 fb             	and    $0xfffffffb,%edx
09 d0                	or     %edx,%eax
89 06                	mov    %eax,(%esi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c3                   	ret    
90                   	nop    

<concatl_sub_bit(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&, int)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 18          	mov    0x18(%esp),%eax
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 14          	mov    0x14(%esp),%esi
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 7c 24 10          	mov    0x10(%esp),%edi
89 1c 24             	mov    %ebx,(%esp)
89 c2                	mov    %eax,%edx
89 c1                	mov    %eax,%ecx
8b 1e                	mov    (%esi),%ebx
c1 fa 1f             	sar    $0x1f,%edx
09 d1                	or     %edx,%ecx
0f 95 c1             	setne  %cl
0f b6 c9             	movzbl %cl,%ecx
83 e3 fd             	and    $0xfffffffd,%ebx
01 c9                	add    %ecx,%ecx
09 d9                	or     %ebx,%ecx
89 0e                	mov    %ecx,(%esi)
0f ac d0 01          	shrd   $0x1,%edx,%eax
d1 ea                	shr    %edx
8b 17                	mov    (%edi),%edx
83 e0 1f             	and    $0x1f,%eax
c1 e0 03             	shl    $0x3,%eax
80 e2 07             	and    $0x7,%dl
09 d0                	or     %edx,%eax
89 07                	mov    %eax,(%edi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
8b 7c 24 08          	mov    0x8(%esp),%edi
83 c4 0c             	add    $0xc,%esp
c3                   	ret    

<concatl_sub_sub(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&, int)>:
83 ec 08             	sub    $0x8,%esp
89 1c 24             	mov    %ebx,(%esp)
8b 5c 24 10          	mov    0x10(%esp),%ebx
8b 54 24 14          	mov    0x14(%esp),%edx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 0c          	mov    0xc(%esp),%esi
8b 0b                	mov    (%ebx),%ecx
89 d0                	mov    %edx,%eax
83 e0 0f             	and    $0xf,%eax
c1 e0 05             	shl    $0x5,%eax
81 e1 1f fe ff ff    	and    $0xfffffe1f,%ecx
09 c8                	or     %ecx,%eax
89 03                	mov    %eax,(%ebx)
8b 06                	mov    (%esi),%eax
d1 ea                	shr    %edx
81 e2 f8 00 00 00    	and    $0xf8,%edx
24 07                	and    $0x7,%al
09 c2                	or     %eax,%edx
89 16                	mov    %edx,(%esi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c3                   	ret    
90                   	nop    

<concatl_sub_int(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&, int)>:
53                   	push   %ebx
8b 44 24 10          	mov    0x10(%esp),%eax
8b 5c 24 08          	mov    0x8(%esp),%ebx
8b 4c 24 0c          	mov    0xc(%esp),%ecx
89 c2                	mov    %eax,%edx
c1 e2 11             	shl    $0x11,%edx
c1 fa 11             	sar    $0x11,%edx
89 11                	mov    %edx,(%ecx)
8b 13                	mov    (%ebx),%edx
c1 e8 0c             	shr    $0xc,%eax
25 f8 00 00 00       	and    $0xf8,%eax
80 e2 07             	and    $0x7,%dl
09 d0                	or     %edx,%eax
89 03                	mov    %eax,(%ebx)
5b                   	pop    %ebx
c3                   	ret    

<concatl_int_bit(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&, int)>:
83 ec 08             	sub    $0x8,%esp
8b 44 24 14          	mov    0x14(%esp),%eax
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 10          	mov    0x10(%esp),%esi
89 1c 24             	mov    %ebx,(%esp)
89 c2                	mov    %eax,%edx
89 c1                	mov    %eax,%ecx
8b 1e                	mov    (%esi),%ebx
c1 fa 1f             	sar    $0x1f,%edx
09 d1                	or     %edx,%ecx
0f 95 c1             	setne  %cl
0f ac d0 01          	shrd   $0x1,%edx,%eax
0f b6 c9             	movzbl %cl,%ecx
d1 ea                	shr    %edx
8b 54 24 0c          	mov    0xc(%esp),%edx
83 e3 fd             	and    $0xfffffffd,%ebx
01 c9                	add    %ecx,%ecx
c1 e0 13             	shl    $0x13,%eax
09 d9                	or     %ebx,%ecx
c1 f8 13             	sar    $0x13,%eax
89 0e                	mov    %ecx,(%esi)
89 02                	mov    %eax,(%edx)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c3                   	ret    
90                   	nop    

<concatl_int_sub(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&, int)>:
53                   	push   %ebx
8b 5c 24 0c          	mov    0xc(%esp),%ebx
8b 4c 24 10          	mov    0x10(%esp),%ecx
8b 13                	mov    (%ebx),%edx
89 c8                	mov    %ecx,%eax
83 e0 0f             	and    $0xf,%eax
c1 e0 05             	shl    $0x5,%eax
81 e2 1f fe ff ff    	and    $0xfffffe1f,%edx
09 d0                	or     %edx,%eax
89 03                	mov    %eax,(%ebx)
8b 44 24 08          	mov    0x8(%esp),%eax
c1 e1 0f             	shl    $0xf,%ecx
c1 f9 13             	sar    $0x13,%ecx
89 08                	mov    %ecx,(%eax)
5b                   	pop    %ebx
c3                   	ret    
90                   	nop    

<concatl_int_int(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&, int)>:
8b 4c 24 0c          	mov    0xc(%esp),%ecx
8b 54 24 08          	mov    0x8(%esp),%edx
89 c8                	mov    %ecx,%eax
c1 e0 11             	shl    $0x11,%eax
c1 f8 11             	sar    $0x11,%eax
89 02                	mov    %eax,(%edx)
8b 44 24 04          	mov    0x4(%esp),%eax
c1 e1 04             	shl    $0x4,%ecx
c1 f9 13             	sar    $0x13,%ecx
89 08                	mov    %ecx,(%eax)
c3                   	ret    
90                   	nop    

<concatl_bit_cat(sc_dt::sc_int<13>&, sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, int)>:
83 ec 0c             	sub    $0xc,%esp
89 1c 24             	mov    %ebx,(%esp)
8b 5c 24 18          	mov    0x18(%esp),%ebx
8b 44 24 1c          	mov    0x1c(%esp),%eax
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 14          	mov    0x14(%esp),%esi
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 7c 24 10          	mov    0x10(%esp),%edi
8b 0b                	mov    (%ebx),%ecx
89 c2                	mov    %eax,%edx
83 e2 07             	and    $0x7,%edx
c1 e2 03             	shl    $0x3,%edx
83 e1 c7             	and    $0xffffffc7,%ecx
09 ca                	or     %ecx,%edx
89 c1                	mov    %eax,%ecx
89 13                	mov    %edx,(%ebx)
8b 1e                	mov    (%esi),%ebx
89 c2                	mov    %eax,%edx
d1 e8                	shr    %eax
25 fc 03 00 00       	and    $0x3fc,%eax
c1 f9 1f             	sar    $0x1f,%ecx
81 e3 03 fc ff ff    	and    $0xfffffc03,%ebx
09 d8                	or     %ebx,%eax
c1 e0 16             	shl    $0x16,%eax
c1 f8 16             	sar    $0x16,%eax
0f ac ca 0b          	shrd   $0xb,%ecx,%edx
c1 e9 0b             	shr    $0xb,%ecx
89 06                	mov    %eax,(%esi)
09 ca                	or     %ecx,%edx
8b 17                	mov    (%edi),%edx
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
c1 e0 02             	shl    $0x2,%eax
83 e2 fb             	and    $0xfffffffb,%edx
09 d0                	or     %edx,%eax
89 07                	mov    %eax,(%edi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
8b 7c 24 08          	mov    0x8(%esp),%edi
83 c4 0c             	add    $0xc,%esp
c3                   	ret    
90                   	nop    

<concatl_sub_cat(sc_dt::sc_int<13>&, sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, int)>:
83 ec 0c             	sub    $0xc,%esp
89 1c 24             	mov    %ebx,(%esp)
8b 5c 24 18          	mov    0x18(%esp),%ebx
8b 54 24 1c          	mov    0x1c(%esp),%edx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 14          	mov    0x14(%esp),%esi
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 7c 24 10          	mov    0x10(%esp),%edi
8b 0b                	mov    (%ebx),%ecx
89 d0                	mov    %edx,%eax
83 e0 07             	and    $0x7,%eax
c1 e0 03             	shl    $0x3,%eax
83 e1 c7             	and    $0xffffffc7,%ecx
09 c8                	or     %ecx,%eax
89 03                	mov    %eax,(%ebx)
8b 0e                	mov    (%esi),%ecx
d1 ea                	shr    %edx
89 d0                	mov    %edx,%eax
25 fc 03 00 00       	and    $0x3fc,%eax
c1 ea 07             	shr    $0x7,%edx
81 e1 03 fc ff ff    	and    $0xfffffc03,%ecx
81 e2 f8 00 00 00    	and    $0xf8,%edx
09 c8                	or     %ecx,%eax
c1 e0 16             	shl    $0x16,%eax
c1 f8 16             	sar    $0x16,%eax
89 06                	mov    %eax,(%esi)
8b 07                	mov    (%edi),%eax
24 07                	and    $0x7,%al
09 c2                	or     %eax,%edx
89 17                	mov    %edx,(%edi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
8b 7c 24 08          	mov    0x8(%esp),%edi
83 c4 0c             	add    $0xc,%esp
c3                   	ret    

<concatl_int_cat(sc_dt::sc_int<13>&, sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, int)>:
83 ec 08             	sub    $0x8,%esp
89 1c 24             	mov    %ebx,(%esp)
8b 5c 24 14          	mov    0x14(%esp),%ebx
8b 4c 24 18          	mov    0x18(%esp),%ecx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 10          	mov    0x10(%esp),%esi
8b 13                	mov    (%ebx),%edx
89 c8                	mov    %ecx,%eax
83 e0 07             	and    $0x7,%eax
c1 e0 03             	shl    $0x3,%eax
83 e2 c7             	and    $0xffffffc7,%edx
09 d0                	or     %edx,%eax
89 03                	mov    %eax,(%ebx)
8b 16                	mov    (%esi),%edx
89 c8                	mov    %ecx,%eax
d1 e8                	shr    %eax
25 fc 03 00 00       	and    $0x3fc,%eax
c1 e1 08             	shl    $0x8,%ecx
81 e2 03 fc ff ff    	and    $0xfffffc03,%edx
09 d0                	or     %edx,%eax
c1 e0 16             	shl    $0x16,%eax
c1 f8 16             	sar    $0x16,%eax
89 06                	mov    %eax,(%esi)
8b 44 24 0c          	mov    0xc(%esp),%eax
c1 f9 13             	sar    $0x13,%ecx
89 08                	mov    %ecx,(%eax)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c3                   	ret    
90                   	nop    

<concatl_cat_bit(sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, sc_dt::sc_int<15>&, int)>:
83 ec 10             	sub    $0x10,%esp
8b 54 24 20          	mov    0x20(%esp),%edx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 1c          	mov    0x1c(%esp),%esi
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 7c 24 18          	mov    0x18(%esp),%edi
89 6c 24 0c          	mov    %ebp,0xc(%esp)
8b 6c 24 14          	mov    0x14(%esp),%ebp
89 d1                	mov    %edx,%ecx
89 d0                	mov    %edx,%eax
89 1c 24             	mov    %ebx,(%esp)
c1 f9 1f             	sar    $0x1f,%ecx
8b 1e                	mov    (%esi),%ebx
09 c8                	or     %ecx,%eax
0f 95 c0             	setne  %al
0f b6 c0             	movzbl %al,%eax
83 e3 fd             	and    $0xfffffffd,%ebx
01 c0                	add    %eax,%eax
09 d8                	or     %ebx,%eax
89 06                	mov    %eax,(%esi)
8b 1f                	mov    (%edi),%ebx
0f ac ca 01          	shrd   $0x1,%ecx,%edx
89 d0                	mov    %edx,%eax
83 e0 07             	and    $0x7,%eax
d1 e9                	shr    %ecx
83 e3 c7             	and    $0xffffffc7,%ebx
c1 e0 03             	shl    $0x3,%eax
09 d8                	or     %ebx,%eax
0f ac ca 03          	shrd   $0x3,%ecx,%edx
89 07                	mov    %eax,(%edi)
8d 04 95 00 00 00 00 	lea    0x0(,%edx,4),%eax
8b 55 00             	mov    0x0(%ebp),%edx
c1 e9 03             	shr    $0x3,%ecx
81 e2 03 fc ff ff    	and    $0xfffffc03,%edx
09 d0                	or     %edx,%eax
c1 e0 16             	shl    $0x16,%eax
c1 f8 16             	sar    $0x16,%eax
89 45 00             	mov    %eax,0x0(%ebp)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
8b 7c 24 08          	mov    0x8(%esp),%edi
8b 6c 24 0c          	mov    0xc(%esp),%ebp
83 c4 10             	add    $0x10,%esp
c3                   	ret    

<concatl_cat_sub(sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, sc_dt::sc_int<15>&, int)>:
83 ec 0c             	sub    $0xc,%esp
89 1c 24             	mov    %ebx,(%esp)
8b 5c 24 18          	mov    0x18(%esp),%ebx
8b 44 24 1c          	mov    0x1c(%esp),%eax
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 14          	mov    0x14(%esp),%esi
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 7c 24 10          	mov    0x10(%esp),%edi
8b 0b                	mov    (%ebx),%ecx
89 c2                	mov    %eax,%edx
83 e2 0f             	and    $0xf,%edx
c1 e2 05             	shl    $0x5,%edx
81 e1 1f fe ff ff    	and    $0xfffffe1f,%ecx
09 ca                	or     %ecx,%edx
89 13                	mov    %edx,(%ebx)
8b 0e                	mov    (%esi),%ecx
d1 e8                	shr    %eax
89 c2                	mov    %eax,%edx
83 e2 38             	and    $0x38,%edx
c1 e8 04             	shr    $0x4,%eax
83 e1 c7             	and    $0xffffffc7,%ecx
25 fc 03 00 00       	and    $0x3fc,%eax
09 ca                	or     %ecx,%edx
89 16                	mov    %edx,(%esi)
8b 17                	mov    (%edi),%edx
81 e2 03 fc ff ff    	and    $0xfffffc03,%edx
09 d0                	or     %edx,%eax
c1 e0 16             	shl    $0x16,%eax
c1 f8 16             	sar    $0x16,%eax
89 07                	mov    %eax,(%edi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
8b 7c 24 08          	mov    0x8(%esp),%edi
83 c4 0c             	add    $0xc,%esp
c3                   	ret    
90                   	nop    

<concatl_cat_int(sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, sc_dt::sc_int<15>&, int)>:
83 ec 08             	sub    $0x8,%esp
8b 44 24 18          	mov    0x18(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
8b 4c 24 14          	mov    0x14(%esp),%ecx
8b 5c 24 10          	mov    0x10(%esp),%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 0c          	mov    0xc(%esp),%esi
89 c2                	mov    %eax,%edx
c1 e2 11             	shl    $0x11,%edx
c1 fa 11             	sar    $0x11,%edx
89 11                	mov    %edx,(%ecx)
8b 0b                	mov    (%ebx),%ecx
c1 e8 0c             	shr    $0xc,%eax
89 c2                	mov    %eax,%edx
83 e2 38             	and    $0x38,%edx
c1 e8 04             	shr    $0x4,%eax
83 e1 c7             	and    $0xffffffc7,%ecx
25 fc 03 00 00       	and    $0x3fc,%eax
09 ca                	or     %ecx,%edx
89 13                	mov    %edx,(%ebx)
8b 16                	mov    (%esi),%edx
81 e2 03 fc ff ff    	and    $0xfffffc03,%edx
09 d0                	or     %edx,%eax
c1 e0 16             	shl    $0x16,%eax
c1 f8 16             	sar    $0x16,%eax
89 06                	mov    %eax,(%esi)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c3                   	ret    

<concatl_cat_cat(sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, sc_dt::sc_int<12>&, sc_dt::sc_int<13>&, int)>:
83 ec 10             	sub    $0x10,%esp
89 1c 24             	mov    %ebx,(%esp)
8b 5c 24 20          	mov    0x20(%esp),%ebx
8b 44 24 24          	mov    0x24(%esp),%eax
89 74 24 04          	mov    %esi,0x4(%esp)
8b 74 24 1c          	mov    0x1c(%esp),%esi
89 7c 24 08          	mov    %edi,0x8(%esp)
8b 7c 24 18          	mov    0x18(%esp),%edi
89 6c 24 0c          	mov    %ebp,0xc(%esp)
8b 0b                	mov    (%ebx),%ecx
89 c2                	mov    %eax,%edx
8b 6c 24 14          	mov    0x14(%esp),%ebp
83 e2 07             	and    $0x7,%edx
c1 e2 03             	shl    $0x3,%edx
83 e1 c7             	and    $0xffffffc7,%ecx
09 ca                	or     %ecx,%edx
89 13                	mov    %edx,(%ebx)
8b 0e                	mov    (%esi),%ecx
89 c2                	mov    %eax,%edx
d1 ea                	shr    %edx
81 e2 fc 03 00 00    	and    $0x3fc,%edx
c1 e8 08             	shr    $0x8,%eax
81 e1 03 fc ff ff    	and    $0xfffffc03,%ecx
09 ca                	or     %ecx,%edx
89 16                	mov    %edx,(%esi)
8b 0f                	mov    (%edi),%ecx
89 c2                	mov    %eax,%edx
83 e2 38             	and    $0x38,%edx
c1 e8 04             	shr    $0x4,%eax
25 fc 03 00 00       	and    $0x3fc,%eax
83 e1 c7             	and    $0xffffffc7,%ecx
09 ca                	or     %ecx,%edx
89 17                	mov    %edx,(%edi)
8b 55 00             	mov    0x0(%ebp),%edx
81 e2 03 fc ff ff    	and    $0xfffffc03,%edx
09 d0                	or     %edx,%eax
c1 e0 16             	shl    $0x16,%eax
c1 f8 16             	sar    $0x16,%eax
89 45 00             	mov    %eax,0x0(%ebp)
8b 1c 24             	mov    (%esp),%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
8b 7c 24 08          	mov    0x8(%esp),%edi
8b 6c 24 0c          	mov    0xc(%esp),%ebp
83 c4 10             	add    $0x10,%esp
c3                   	ret    
90                   	nop    

<concat_bit_bit(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&)>:
83 ec 08             	sub    $0x8,%esp
31 d2                	xor    %edx,%edx
8b 44 24 0c          	mov    0xc(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
8b 1c 24             	mov    (%esp),%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
83 e0 01             	and    $0x1,%eax
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
8b 31                	mov    (%ecx),%esi
d1 fe                	sar    %esi
89 f1                	mov    %esi,%ecx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
83 e1 01             	and    $0x1,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bit_sub(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
31 d2                	xor    %edx,%edx
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f a4 c2 04          	shld   $0x4,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 04             	shl    $0x4,%eax
c1 f9 05             	sar    $0x5,%ecx
83 e1 0f             	and    $0xf,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bit_int(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
31 d2                	xor    %edx,%edx
8b 08                	mov    (%eax),%ecx
c1 f9 02             	sar    $0x2,%ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
83 e0 01             	and    $0x1,%eax
5b                   	pop    %ebx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 0f             	shl    $0xf,%eax
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_sub_bit(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&)>:
83 ec 08             	sub    $0x8,%esp
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 4c 24 10          	mov    0x10(%esp),%ecx
89 1c 24             	mov    %ebx,(%esp)
8b 1c 24             	mov    (%esp),%ebx
89 74 24 04          	mov    %esi,0x4(%esp)
8b 00                	mov    (%eax),%eax
8b 31                	mov    (%ecx),%esi
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
d1 fe                	sar    %esi
89 c2                	mov    %eax,%edx
89 f1                	mov    %esi,%ecx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 08             	add    $0x8,%esp
c1 fa 1f             	sar    $0x1f,%edx
83 e1 01             	and    $0x1,%ecx
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_sub_sub(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
c1 f9 05             	sar    $0x5,%ecx
0f a4 c2 04          	shld   $0x4,%eax,%edx
83 e1 0f             	and    $0xf,%ecx
c1 e0 04             	shl    $0x4,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_sub_int(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
c1 e0 0f             	shl    $0xf,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_int_bit(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 08                	mov    (%eax),%ecx
d1 f9                	sar    %ecx
89 c8                	mov    %ecx,%eax
8b 4c 24 08          	mov    0x8(%esp),%ecx
83 e0 01             	and    $0x1,%eax
8b 09                	mov    (%ecx),%ecx
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 01          	shld   $0x1,%ecx,%ebx
01 c9                	add    %ecx,%ecx
5b                   	pop    %ebx
09 c8                	or     %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_int_sub(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 08                	mov    (%eax),%ecx
8b 44 24 0c          	mov    0xc(%esp),%eax
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
8b 00                	mov    (%eax),%eax
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 04          	shld   $0x4,%ecx,%ebx
c1 e1 04             	shl    $0x4,%ecx
c1 f8 05             	sar    $0x5,%eax
83 e0 0f             	and    $0xf,%eax
5b                   	pop    %ebx
09 c1                	or     %eax,%ecx
89 c8                	mov    %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_int_int(sc_dt::sc_int<13>&, sc_dt::sc_int<15>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 4c 24 0c          	mov    0xc(%esp),%ecx
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
8b 09                	mov    (%ecx),%ecx
25 ff 1f 00 00       	and    $0x1fff,%eax
89 c2                	mov    %eax,%edx
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
c1 e0 0f             	shl    $0xf,%eax
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_bit_cat(sc_dt::sc_int<13>&, sc_dt::sc_int<10>&, sc_dt::sc_int<11>&)>:
56                   	push   %esi
31 d2                	xor    %edx,%edx
53                   	push   %ebx
8b 44 24 10          	mov    0x10(%esp),%eax
8b 08                	mov    (%eax),%ecx
8b 44 24 14          	mov    0x14(%esp),%eax
c1 f9 02             	sar    $0x2,%ecx
8b 00                	mov    (%eax),%eax
81 e1 ff 00 00 00    	and    $0xff,%ecx
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 03          	shld   $0x3,%ecx,%ebx
c1 f8 03             	sar    $0x3,%eax
83 e0 07             	and    $0x7,%eax
c1 e1 03             	shl    $0x3,%ecx
09 c1                	or     %eax,%ecx
8b 44 24 0c          	mov    0xc(%esp),%eax
5b                   	pop    %ebx
8b 30                	mov    (%eax),%esi
c1 fe 02             	sar    $0x2,%esi
89 f0                	mov    %esi,%eax
83 e0 01             	and    $0x1,%eax
0f a4 c2 0b          	shld   $0xb,%eax,%edx
5e                   	pop    %esi
c1 e0 0b             	shl    $0xb,%eax
09 c1                	or     %eax,%ecx
89 c8                	mov    %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_sub_cat(sc_dt::sc_int<13>&, sc_dt::sc_int<10>&, sc_dt::sc_int<11>&)>:
53                   	push   %ebx
8b 44 24 0c          	mov    0xc(%esp),%eax
8b 08                	mov    (%eax),%ecx
8b 44 24 08          	mov    0x8(%esp),%eax
c1 f9 02             	sar    $0x2,%ecx
8b 00                	mov    (%eax),%eax
81 e1 ff 00 00 00    	and    $0xff,%ecx
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 03          	shld   $0x3,%ecx,%ebx
c1 f8 03             	sar    $0x3,%eax
83 e0 1f             	and    $0x1f,%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
c1 e1 03             	shl    $0x3,%ecx
0f a4 c2 0b          	shld   $0xb,%eax,%edx
c1 e0 0b             	shl    $0xb,%eax
09 c1                	or     %eax,%ecx
8b 44 24 10          	mov    0x10(%esp),%eax
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 07             	and    $0x7,%eax
09 c1                	or     %eax,%ecx
89 c8                	mov    %ecx,%eax
c3                   	ret    

<concat_int_cat(sc_dt::sc_int<13>&, sc_dt::sc_int<10>&, sc_dt::sc_int<11>&)>:
53                   	push   %ebx
8b 44 24 08          	mov    0x8(%esp),%eax
8b 08                	mov    (%eax),%ecx
8b 44 24 10          	mov    0x10(%esp),%eax
81 e1 ff 1f 00 00    	and    $0x1fff,%ecx
8b 00                	mov    (%eax),%eax
89 cb                	mov    %ecx,%ebx
c1 fb 1f             	sar    $0x1f,%ebx
0f a4 cb 0b          	shld   $0xb,%ecx,%ebx
c1 e1 0b             	shl    $0xb,%ecx
c1 f8 03             	sar    $0x3,%eax
83 e0 07             	and    $0x7,%eax
09 c1                	or     %eax,%ecx
8b 44 24 0c          	mov    0xc(%esp),%eax
5b                   	pop    %ebx
8b 00                	mov    (%eax),%eax
c1 f8 02             	sar    $0x2,%eax
25 ff 00 00 00       	and    $0xff,%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
0f a4 c2 03          	shld   $0x3,%eax,%edx
c1 e0 03             	shl    $0x3,%eax
09 c1                	or     %eax,%ecx
89 c8                	mov    %ecx,%eax
c3                   	ret    
90                   	nop    

<concat_cat_bit(sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, sc_dt::sc_int<15>&)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 10          	mov    0x10(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
89 7c 24 08          	mov    %edi,0x8(%esp)
89 74 24 04          	mov    %esi,0x4(%esp)
8b 30                	mov    (%eax),%esi
8b 44 24 14          	mov    0x14(%esp),%eax
c1 fe 02             	sar    $0x2,%esi
8b 08                	mov    (%eax),%ecx
81 e6 ff 00 00 00    	and    $0xff,%esi
89 f7                	mov    %esi,%edi
c1 ff 1f             	sar    $0x1f,%edi
0f a4 f7 03          	shld   $0x3,%esi,%edi
c1 f9 03             	sar    $0x3,%ecx
89 fa                	mov    %edi,%edx
8b 7c 24 08          	mov    0x8(%esp),%edi
c1 e6 03             	shl    $0x3,%esi
83 e1 07             	and    $0x7,%ecx
89 f0                	mov    %esi,%eax
89 cb                	mov    %ecx,%ebx
09 c8                	or     %ecx,%eax
8b 4c 24 18          	mov    0x18(%esp),%ecx
c1 fb 1f             	sar    $0x1f,%ebx
09 da                	or     %ebx,%edx
8b 1c 24             	mov    (%esp),%ebx
0f a4 c2 01          	shld   $0x1,%eax,%edx
01 c0                	add    %eax,%eax
8b 31                	mov    (%ecx),%esi
d1 fe                	sar    %esi
89 f1                	mov    %esi,%ecx
8b 74 24 04          	mov    0x4(%esp),%esi
83 c4 0c             	add    $0xc,%esp
83 e1 01             	and    $0x1,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_cat_sub(sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, sc_dt::sc_int<15>&)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 10          	mov    0x10(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
89 7c 24 08          	mov    %edi,0x8(%esp)
89 74 24 04          	mov    %esi,0x4(%esp)
8b 30                	mov    (%eax),%esi
8b 44 24 14          	mov    0x14(%esp),%eax
c1 fe 02             	sar    $0x2,%esi
8b 08                	mov    (%eax),%ecx
81 e6 ff 00 00 00    	and    $0xff,%esi
89 f7                	mov    %esi,%edi
c1 ff 1f             	sar    $0x1f,%edi
0f a4 f7 03          	shld   $0x3,%esi,%edi
c1 f9 03             	sar    $0x3,%ecx
89 fa                	mov    %edi,%edx
8b 7c 24 08          	mov    0x8(%esp),%edi
c1 e6 03             	shl    $0x3,%esi
83 e1 07             	and    $0x7,%ecx
89 f0                	mov    %esi,%eax
89 cb                	mov    %ecx,%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
09 c8                	or     %ecx,%eax
8b 4c 24 18          	mov    0x18(%esp),%ecx
c1 fb 1f             	sar    $0x1f,%ebx
09 da                	or     %ebx,%edx
8b 1c 24             	mov    (%esp),%ebx
0f a4 c2 04          	shld   $0x4,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 04             	shl    $0x4,%eax
83 c4 0c             	add    $0xc,%esp
c1 f9 05             	sar    $0x5,%ecx
83 e1 0f             	and    $0xf,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_cat_int(sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, sc_dt::sc_int<15>&)>:
83 ec 0c             	sub    $0xc,%esp
8b 44 24 10          	mov    0x10(%esp),%eax
89 1c 24             	mov    %ebx,(%esp)
89 7c 24 08          	mov    %edi,0x8(%esp)
89 74 24 04          	mov    %esi,0x4(%esp)
8b 30                	mov    (%eax),%esi
8b 44 24 14          	mov    0x14(%esp),%eax
c1 fe 02             	sar    $0x2,%esi
8b 08                	mov    (%eax),%ecx
81 e6 ff 00 00 00    	and    $0xff,%esi
89 f7                	mov    %esi,%edi
c1 ff 1f             	sar    $0x1f,%edi
0f a4 f7 03          	shld   $0x3,%esi,%edi
c1 f9 03             	sar    $0x3,%ecx
89 fa                	mov    %edi,%edx
8b 7c 24 08          	mov    0x8(%esp),%edi
c1 e6 03             	shl    $0x3,%esi
83 e1 07             	and    $0x7,%ecx
89 f0                	mov    %esi,%eax
89 cb                	mov    %ecx,%ebx
8b 74 24 04          	mov    0x4(%esp),%esi
09 c8                	or     %ecx,%eax
8b 4c 24 18          	mov    0x18(%esp),%ecx
c1 fb 1f             	sar    $0x1f,%ebx
09 da                	or     %ebx,%edx
8b 1c 24             	mov    (%esp),%ebx
0f a4 c2 0f          	shld   $0xf,%eax,%edx
8b 09                	mov    (%ecx),%ecx
c1 e0 0f             	shl    $0xf,%eax
83 c4 0c             	add    $0xc,%esp
81 e1 ff 7f 00 00    	and    $0x7fff,%ecx
09 c8                	or     %ecx,%eax
c3                   	ret    

<concat_cat_cat(sc_dt::sc_int<10>&, sc_dt::sc_int<11>&, sc_dt::sc_int<12>&, sc_dt::sc_int<13>&)>:
83 ec 14             	sub    $0x14,%esp
8b 44 24 20          	mov    0x20(%esp),%eax
89 5c 24 08          	mov    %ebx,0x8(%esp)
89 7c 24 10          	mov    %edi,0x10(%esp)
89 74 24 0c          	mov    %esi,0xc(%esp)
8b 30                	mov    (%eax),%esi
8b 44 24 24          	mov    0x24(%esp),%eax
c1 fe 02             	sar    $0x2,%esi
8b 00                	mov    (%eax),%eax
81 e6 ff 00 00 00    	and    $0xff,%esi
89 f7                	mov    %esi,%edi
c1 ff 1f             	sar    $0x1f,%edi
0f a4 f7 03          	shld   $0x3,%esi,%edi
8b 7c 24 10          	mov    0x10(%esp),%edi
c1 f8 03             	sar    $0x3,%eax
83 e0 07             	and    $0x7,%eax
c1 e6 03             	shl    $0x3,%esi
09 c6                	or     %eax,%esi
8b 44 24 18          	mov    0x18(%esp),%eax
8b 00                	mov    (%eax),%eax
c1 f8 02             	sar    $0x2,%eax
25 ff 00 00 00       	and    $0xff,%eax
89 c2                	mov    %eax,%edx
c1 fa 1f             	sar    $0x1f,%edx
89 04 24             	mov    %eax,(%esp)
8b 04 24             	mov    (%esp),%eax
89 54 24 04          	mov    %edx,0x4(%esp)
8b 54 24 04          	mov    0x4(%esp),%edx
0f a4 c2 03          	shld   $0x3,%eax,%edx
c1 e0 03             	shl    $0x3,%eax
89 04 24             	mov    %eax,(%esp)
8b 44 24 1c          	mov    0x1c(%esp),%eax
89 54 24 04          	mov    %edx,0x4(%esp)
8b 0c 24             	mov    (%esp),%ecx
8b 5c 24 04          	mov    0x4(%esp),%ebx
8b 00                	mov    (%eax),%eax
c1 f8 03             	sar    $0x3,%eax
83 e0 07             	and    $0x7,%eax
89 c2                	mov    %eax,%edx
09 c1                	or     %eax,%ecx
c1 fa 1f             	sar    $0x1f,%edx
09 d3                	or     %edx,%ebx
0f a4 cb 0b          	shld   $0xb,%ecx,%ebx
8b 5c 24 08          	mov    0x8(%esp),%ebx
c1 e1 0b             	shl    $0xb,%ecx
09 ce                	or     %ecx,%esi
89 f0                	mov    %esi,%eax
8b 74 24 0c          	mov    0xc(%esp),%esi
83 c4 14             	add    $0x14,%esp
c3                   	ret    
